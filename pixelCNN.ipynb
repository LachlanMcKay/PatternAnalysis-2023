{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEVICE: cuda\n"
     ]
    }
   ],
   "source": [
    "# ==== import from package ==== #\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from einops import rearrange, repeat, reduce\n",
    "from collections import defaultdict\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "# ==== import from this folder ==== #\n",
    "from model_VAE import VQVAE\n",
    "from model_discriminator import NLayerDiscriminator, weights_init\n",
    "from dataset import get_dataloader\n",
    "from util import reset_dir, weight_scheduler, compact_large_image, sinusoidal_embedding\n",
    "from logger import Logger\n",
    "DEVICE = torch.device(\"cuda\")\n",
    "print(\"DEVICE:\", DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([11328, 256])\n"
     ]
    }
   ],
   "source": [
    "mode = 'VQVAE'\n",
    "load_epoch = 45\n",
    "\n",
    "# Get latent set\n",
    "latent_set = torch.load(f'collected_latents/{mode}_{load_epoch}.pt')\n",
    "vae = torch.load(f'model_ckpt/{mode}/epoch_AE_{load_epoch}.pt')\n",
    "vae.eval()\n",
    "\n",
    "# Try to normalize latents with conditions\n",
    "with torch.no_grad():\n",
    "    latents, z_indices = latent_set.tensors[0], latent_set.tensors[1]\n",
    "    quant, diff_loss, ind = vae.quantize(latents)\n",
    "    ind = rearrange(ind, '(b 1 h w) -> b (h w)', b=len(latents),\n",
    "                            h=vae.z_shape[0], w=vae.z_shape[1])\n",
    "    dataset = TensorDataset(ind, z_indices)\n",
    "    dataloader = DataLoader(dataset=dataset, batch_size=4, shuffle=True)\n",
    "    print(ind.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PixelCNN(nn.Module):\n",
    "\n",
    "    def __init__(self, vae):\n",
    "        super(PixelCNN, self).__init__()\n",
    "\n",
    "        self.embed_dim = 32\n",
    "        self.n_embed = vae.quantize.n_e\n",
    "        self.z_shape = vae.z_shape\n",
    "\n",
    "        self.embed = nn.Embedding(vae.quantize.n_e, self.embed_dim)\n",
    "        decoder_layer = nn.TransformerDecoderLayer(d_model=self.embed_dim, nhead=2)\n",
    "        self.decoder = nn.TransformerDecoder(decoder_layer, num_layers=2)\n",
    "        self.positional_encoding = sinusoidal_embedding(self.z_shape[0] * self.z_shape[1], self.embed_dim)\n",
    "        self.predictor = nn.Sequential(\n",
    "            nn.Linear(self.embed_dim, self.embed_dim),\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(self.embed_dim, self.n_embed),\n",
    "        )\n",
    "        self.cond_mlp = nn.Sequential(\n",
    "            nn.Embedding(32, self.embed_dim),\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(self.embed_dim, self.embed_dim),\n",
    "        )\n",
    "\n",
    "    def forward(self, dst, z_idx):\n",
    "        N, S = dst.shape\n",
    "        \n",
    "        h = self.embed(dst) + self.positional_encoding[None, :, :].to(dst.device)\n",
    "        memory = self.cond_mlp(z_idx)\n",
    "        mask = nn.Transformer.generate_square_subsequent_mask(S).to(DEVICE)\n",
    "        memory = repeat(memory, 'N E -> S N E', S=S)\n",
    "        # Add gaussian noise\n",
    "        memory = memory + torch.randn_like(memory)\n",
    "        h = rearrange(h, 'N S E -> S N E')\n",
    "        h = self.decoder(h, memory, tgt_mask=mask)\n",
    "        h = self.predictor(h)\n",
    "        h = rearrange(h, 'S N E-> N S E')\n",
    "        return h\n",
    "    \n",
    "    def sample(self, batch_size):\n",
    "        N, S, E = batch_size, 16*16, self.embed_dim\n",
    "        T = 32\n",
    "        # 32, E -> 32 * B * S, E\n",
    "        memory = torch.randn([N, T, E]).to(DEVICE)\n",
    "        z_idx = torch.arange(0, 32, step=1, device=DEVICE)\n",
    "        cond = self.cond_mlp(z_idx)\n",
    "        memory = memory + cond[None, :, :]\n",
    "        memory = repeat(memory, 'N T E -> S (N T) E', S=S)\n",
    "\n",
    "        pos_enc = self.positional_encoding[None, :, :].to(DEVICE)\n",
    "        mask = nn.Transformer.generate_square_subsequent_mask(S).to(DEVICE)\n",
    "\n",
    "        gen_ind = torch.zeros(S, N * T).long().to(DEVICE)\n",
    "        for i in tqdm(range(S), total=S):\n",
    "            rev_ind = rearrange(gen_ind, 'S (N T) -> (N T) S', N=N, T=T)\n",
    "            h = self.embed(rev_ind) + pos_enc\n",
    "            h = rearrange(h, '(N T) S E -> S (N T) E',N=N, T=T)\n",
    "            h = self.decoder(h, memory, tgt_mask=mask)\n",
    "            h = self.predictor(h)\n",
    "            h = rearrange(h, 'S (N T) E-> (N T) S E', N=N, T=T)\n",
    "            from torch.utils.data import WeightedRandomSampler\n",
    "            h = h[:, i, :].detach()\n",
    "            cur_gen_ind = torch.zeros([N * T, ]).long().to(DEVICE)\n",
    "            for idx in range(N * T):\n",
    "                weight = h[idx].detach().cpu()\n",
    "                weight = torch.nn.functional.softmax(weight+1e-6, dim=0)\n",
    "                sample = WeightedRandomSampler(weight, num_samples=1)\n",
    "                sample = list(sample)[0]\n",
    "                cur_gen_ind[idx] = sample\n",
    "\n",
    "            # h = torch.argmax(h, dim=1)\n",
    "            gen_ind[i] = cur_gen_ind\n",
    "        return rearrange(gen_ind, 'S (N T) -> N T S', N=N, T=T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "pixelCNN = PixelCNN(vae).to(DEVICE)\n",
    "from torchinfo import summary\n",
    "criteria = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(pixelCNN.parameters(), lr=2e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(i):\n",
    "    total_loss = 0\n",
    "    for now_step, batch_data in tqdm(enumerate(dataloader), total=len(dataloader)):\n",
    "        ind, z_idx = [data.to(DEVICE) for data in batch_data]\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        out = pixelCNN(ind.detach(), z_idx.detach())\n",
    "        out = rearrange(out, \"N S E -> (N S) E\")\n",
    "        ind = rearrange(ind, \"N S -> (N S)\")\n",
    "        loss = criteria(out, ind)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "    return total_loss / len(dataloader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2832/2832 [01:07<00:00, 42.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0: loss: 0.3086366430558014\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2832/2832 [01:11<00:00, 39.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1: loss: 0.0006204869874112527\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2832/2832 [01:15<00:00, 37.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 2: loss: 0.0001256646134663376\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2832/2832 [01:13<00:00, 38.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 3: loss: 4.674267041783593e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2832/2832 [01:12<00:00, 39.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 4: loss: 2.358297165013106e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    print(f\"epoch {i}: loss: {train(i)}\")\n",
    "    torch.save(pixelCNN, f'model_ckpt/visualTransformer/model_{i}.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 256/256 [00:10<00:00, 23.86it/s]\n",
      "100%|██████████| 256/256 [00:09<00:00, 25.67it/s]\n",
      "100%|██████████| 256/256 [00:11<00:00, 22.51it/s]\n",
      "100%|██████████| 256/256 [00:10<00:00, 25.24it/s]\n",
      "100%|██████████| 256/256 [00:10<00:00, 24.99it/s]\n",
      "100%|██████████| 256/256 [00:09<00:00, 25.97it/s]\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    from einops import repeat, rearrange\n",
    "    for sample_idx in range(6):\n",
    "        out_inds = pixelCNN.sample(1)[0]\n",
    "        z_q = vae.quantize.embedding(out_inds)\n",
    "        z_q = rearrange(z_q, 'b (h w) c -> b c h w', w = 16)\n",
    "        cond = torch.arange(0, 32, 1).long().to(DEVICE)\n",
    "        sample = vae.decode(z_q, cond).cpu()\n",
    "        from util import compact_large_image\n",
    "        imgs = compact_large_image(sample, HZ=4, WZ=8)\n",
    "        for idx in range(imgs.shape[0]):\n",
    "            plt.imsave(f'visualize/Transformer_vis/{sample_idx}.png', imgs[idx] * 0.5 + 0.5, cmap='gray')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
