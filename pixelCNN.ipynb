{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEVICE: cuda\n"
     ]
    }
   ],
   "source": [
    "# ==== import from package ==== #\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from einops import rearrange, repeat, reduce\n",
    "from collections import defaultdict\n",
    "# ==== import from this folder ==== #\n",
    "from model_VQVAE import VQVAE\n",
    "from discriminator import NLayerDiscriminator, weights_init\n",
    "from dataset import get_dataloader\n",
    "from util import reset_dir, weight_scheduler, compact_large_image\n",
    "from logger import Logger\n",
    "DEVICE = torch.device(\"cuda\")\n",
    "print(\"DEVICE:\", DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "called train_and_validate 32 None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9664/9664 [00:11<00:00, 811.24it/s]\n",
      "100%|██████████| 1120/1120 [00:01<00:00, 772.98it/s]\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "dataloader = get_dataloader(mode='train_and_validate', batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "net = VQVAE().to(DEVICE)\n",
    "net = torch.load(f'model_ckpt/VQVAE/epoch_AE_50.pt')\n",
    "net.eval()\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ind_data(net, dataloader):\n",
    "    indices = []\n",
    "    for now_step, batch_data in tqdm(enumerate(dataloader), total=len(dataloader)):\n",
    "        raw_img, seg_img, brain_idx, z_idx = [\n",
    "            data.to(DEVICE) for data in batch_data]\n",
    "        with torch.no_grad():\n",
    "            batch_size = raw_img.shape[0]\n",
    "            latent = net.encode(raw_img)\n",
    "            quant, diff_loss, (_, _, ind) = net.quantize(latent)\n",
    "            ind = rearrange(ind, '(b c h w) -> b c h w', b=batch_size,\n",
    "                            h=net.z_shape[0], w=net.z_shape[1])\n",
    "            indices.append(ind.detach())\n",
    "    indices = torch.cat(indices)\n",
    "    indices = reduce(indices, 'b c h w -> b h w', 'min')\n",
    "    return indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 337/337 [01:16<00:00,  4.38it/s]\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    ind = get_ind_data(net, dataloader).detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10784, 16, 16])\n",
      "tensor(252, device='cuda:0') tensor(17, device='cuda:0')\n",
      "   8   6   4   6   6   6   6   6   6   7  10  11   6   6   7   6\n",
      "   4   3   6   7   7   7   7   7   7   6   6   5   5   8   6   6\n",
      "   5   6   8   8   7   7   6   7   7   6   8   7   6   9   6   6\n",
      "   7   8   7   7   7   8   7   6   7   7   7   7   8   8   7   6\n",
      "   8   8   7   6   7   8   8   6   6   8   8   8   9   7   6   7\n",
      "   9   9   6   8   8   6   6   6   6   8   8   7   6   6   7   8\n",
      "   8   7   7   6   7   6   7   6   6   6   6   6   8   8   7   8\n",
      "   7   5   7   8   6   7   7   6   6   6   6   6   6   7   7   7\n",
      "   7   6   7   6   6   6   6   6   6   6   6   7   7   6   6   8\n",
      "   6   5   8   6   6   6   6   6   6   5   6   7   8   8   7   7\n",
      "   7   5   7   8   6   6   6   7   6   7   6   6   7   8   6   9\n",
      "   9   7   8   8   8   6   8   6   7   7   7   7   7   6   7   9\n",
      "   8   9   8   7   7   7   6   6   7   6   8   7   9   7   9   8\n",
      "   8   7   8   8   7   8   7   7   7   8   8   8   9   8  10   7\n",
      "   8   7   7   8   8   8   9   8   8   8   8   8   8   8   9   9\n",
      "   8   9   5   7   7   9   8   8   8   8   6   7  10  10  10  10\n"
     ]
    }
   ],
   "source": [
    "print(ind.shape)\n",
    "print(torch.max(ind), torch.min(ind))\n",
    "for i in range(16):\n",
    "    for j in range(16):\n",
    "        x = ind[:, i, j]\n",
    "        print('%4d' % len(torch.unique(x)), end='')\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10784, 16, 16])\n",
      "torch.Size([10784, 16, 16, 8])\n"
     ]
    }
   ],
   "source": [
    "# ind = rearrange(ind[:27], 'b w h -> b w h ')\n",
    "print(ind.shape)\n",
    "# ind shape: [batch_size, w*h]\n",
    "emb = net.quantize.embedding(ind)\n",
    "# emb shape: [batch_size, w*h, emb_len]\n",
    "print(emb.shape)\n",
    "\n",
    "ind = rearrange(ind, 'N W H -> N (W H)')\n",
    "emb = rearrange(emb, 'N W H E -> N ( W H ) E')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "class PositionalEncoding(nn.Module):\n",
    "    \"Implement the PE function.\"\n",
    "\n",
    "    def __init__(self, d_model, dropout, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        # 初始化Shape为(max_len, d_model)的PE (positional encoding)\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        # 初始化一个tensor [[0, 1, 2, 3, ...]]\n",
    "        position = torch.arange(0, max_len).unsqueeze(1)\n",
    "        # 这里就是sin和cos括号中的内容，通过e和ln进行了变换\n",
    "        div_term = torch.exp(\n",
    "            torch.arange(0, d_model, 2) * -(math.log(10000.0) / d_model)\n",
    "        )\n",
    "        # 计算PE(pos, 2i)\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        # 计算PE(pos, 2i+1)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        # 为了方便计算，在最外面在unsqueeze出一个batch\n",
    "        pe = pe.unsqueeze(0)\n",
    "        # 如果一个参数不参与梯度下降，但又希望保存model的时候将其保存下来\n",
    "        # 这个时候就可以用register_buffer\n",
    "        self.register_buffer(\"pe\", pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x 为embedding后的inputs，例如(1,7, 128)，batch size为1,7个单词，单词维度为128\n",
    "        \"\"\"\n",
    "        # 将x和positional encoding相加。\n",
    "        x = x + self.pe[:, : x.size(1)].requires_grad_(False)\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PixelCNN(nn.Module):\n",
    "\n",
    "    def __init__(self, vae):\n",
    "        super(PixelCNN, self).__init__()\n",
    "\n",
    "        self.embed_dim = vae.quantize.e_dim\n",
    "        self.n_embed = vae.quantize.n_e\n",
    "        self.z_shape = vae.z_shape\n",
    "        self.transformer = nn.Transformer(d_model=self.embed_dim,\n",
    "                                     nhead=8,\n",
    "                                     num_encoder_layers=6,\n",
    "                                     num_decoder_layers=6,\n",
    "                                     dim_feedforward=256,\n",
    "                                     dropout=0.1,\n",
    "                                     batch_first=True).cuda()\n",
    "        \n",
    "        self.positional_encoding = PositionalEncoding(d_model=self.embed_dim, max_len=self.z_shape[0] * self.z_shape[1], dropout=0)\n",
    "\n",
    "        self.predictor = nn.Linear(self.embed_dim, self.n_embed)\n",
    "\n",
    "    def generate_random_mask(self, batch_size, S, low=None, high=None):\n",
    "        if low is None:\n",
    "            low = 0\n",
    "        if high is None:\n",
    "            high = S+1\n",
    "        mask = torch.zeros([batch_size, S], device=DEVICE, dtype=torch.long)\n",
    "        for i in range(batch_size):\n",
    "            mask_num = torch.randint(low=low, high=high, size=(1,),  device=DEVICE)\n",
    "            perm = torch.randperm(S, device=DEVICE)[None, :]\n",
    "            mask[i, :] = (perm < mask_num).long()\n",
    "        return mask\n",
    "\n",
    "    def forward(self, src, low=None, high=None):\n",
    "        N, S, E = src.shape\n",
    "        tgt_mask = nn.Transformer.generate_square_subsequent_mask(S).cuda()\n",
    "        from einops import repeat, rearrange\n",
    "\n",
    "        teacher_forcing = False\n",
    "        tgt = src\n",
    "        src_mask = self.generate_random_mask(batch_size=N, S=S, low=low, high=high)\n",
    "        src = src * (1 - src_mask[:, :, None].long())\n",
    "        src = self.positional_encoding(src)\n",
    "        tgt = self.positional_encoding(tgt)\n",
    "        if teacher_forcing:\n",
    "            out = self.transformer(src, tgt, tgt_mask=tgt_mask)\n",
    "            out = self.predictor(out)\n",
    "            return out\n",
    "        else:\n",
    "            src_enc = self.transformer.encoder(src)\n",
    "            out_embs = torch.zeros([N, S, E], device=DEVICE)\n",
    "            out_inds = []\n",
    "            out_logits = []\n",
    "            for i in range(S):\n",
    "                # print(cur[0, :,0])\n",
    "                print(i, out_embs.shape, src_enc.shape)\n",
    "                mask = repeat(tgt_mask[i], 'S -> N S', N = N)\n",
    "\n",
    "                out = self.transformer.decoder(out_embs, memory=src_enc, tgt_mask=mask)\n",
    "                logits = self.predictor(out[:, i, :])\n",
    "                out_logits.append(logits[:, None, :])\n",
    "                out_ind = torch.argmax(logits.detach(), dim=1)\n",
    "                out_inds.append(out_ind[:, None])\n",
    "                out_emb = net.quantize.embedding(out_ind).detach()\n",
    "                out_embs[:, i, :] = out_emb\n",
    "            return torch.cat(out_logits, dim=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "pixelCNN = PixelCNN(net).to(DEVICE)\n",
    "from torchinfo import summary\n",
    "criteria = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(pixelCNN.parameters(), lr=2e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10784, 256]) torch.Size([10784, 256, 8])\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "print(ind.shape, emb.shape)\n",
    "dataset = TensorDataset(emb, ind)\n",
    "dataloader = DataLoader(dataset=dataset, batch_size=1, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(i):\n",
    "\n",
    "    cut = min(i // 2, 15)\n",
    "    low, high = cut*16, (cut+1) * 16\n",
    "    print(low, high)\n",
    "    total_loss = 0\n",
    "    for now_step, batch_data in tqdm(enumerate(dataloader), total=len(dataloader)):\n",
    "        emb, ind = [data.to(DEVICE) for data in batch_data]\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        out = pixelCNN(emb.detach(), low=low, high=high)\n",
    "        loss = criteria(out.contiguous().view(-1, out.size(-1)), ind.contiguous().view(-1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "    return total_loss / len(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10784 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 torch.Size([1, 256, 8]) torch.Size([1, 256, 8])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The shape of the 2D attn_mask is torch.Size([256, 1]), but should be (256, 256).",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32md:\\class\\tutor\\kasper_ML\\hw3\\check_ind.ipynb Cell 13\u001b[0m line \u001b[0;36m2\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/class/tutor/kasper_ML/hw3/check_ind.ipynb#X15sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m50\u001b[39m):\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/class/tutor/kasper_ML/hw3/check_ind.ipynb#X15sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mepoch \u001b[39m\u001b[39m{\u001b[39;00mi\u001b[39m}\u001b[39;00m\u001b[39m: loss: \u001b[39m\u001b[39m{\u001b[39;00mtrain(i)\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "\u001b[1;32md:\\class\\tutor\\kasper_ML\\hw3\\check_ind.ipynb Cell 13\u001b[0m line \u001b[0;36m1\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/class/tutor/kasper_ML/hw3/check_ind.ipynb#X15sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m emb, ind \u001b[39m=\u001b[39m [data\u001b[39m.\u001b[39mto(DEVICE) \u001b[39mfor\u001b[39;00m data \u001b[39min\u001b[39;00m batch_data]\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/class/tutor/kasper_ML/hw3/check_ind.ipynb#X15sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/class/tutor/kasper_ML/hw3/check_ind.ipynb#X15sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m out \u001b[39m=\u001b[39m pixelCNN(emb\u001b[39m.\u001b[39;49mdetach(), low\u001b[39m=\u001b[39;49mlow, high\u001b[39m=\u001b[39;49mhigh)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/class/tutor/kasper_ML/hw3/check_ind.ipynb#X15sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m loss \u001b[39m=\u001b[39m criteria(out\u001b[39m.\u001b[39mcontiguous()\u001b[39m.\u001b[39mview(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, out\u001b[39m.\u001b[39msize(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)), ind\u001b[39m.\u001b[39mcontiguous()\u001b[39m.\u001b[39mview(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m))\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/class/tutor/kasper_ML/hw3/check_ind.ipynb#X15sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m loss\u001b[39m.\u001b[39mbackward()\n",
      "File \u001b[1;32mc:\\Python39\\lib\\site-packages\\torch\\nn\\modules\\module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1106\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1107\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1108\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1109\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1110\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1111\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\u001b[1;32md:\\class\\tutor\\kasper_ML\\hw3\\check_ind.ipynb Cell 13\u001b[0m line \u001b[0;36m5\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/class/tutor/kasper_ML/hw3/check_ind.ipynb#X15sZmlsZQ%3D%3D?line=54'>55</a>\u001b[0m \u001b[39mprint\u001b[39m(i, out_embs\u001b[39m.\u001b[39mshape, src_enc\u001b[39m.\u001b[39mshape)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/class/tutor/kasper_ML/hw3/check_ind.ipynb#X15sZmlsZQ%3D%3D?line=55'>56</a>\u001b[0m mask \u001b[39m=\u001b[39m repeat(tgt_mask[i], \u001b[39m'\u001b[39m\u001b[39mS -> N S\u001b[39m\u001b[39m'\u001b[39m, N \u001b[39m=\u001b[39m N)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/class/tutor/kasper_ML/hw3/check_ind.ipynb#X15sZmlsZQ%3D%3D?line=57'>58</a>\u001b[0m out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtransformer\u001b[39m.\u001b[39;49mdecoder(out_embs, memory\u001b[39m=\u001b[39;49msrc_enc, tgt_mask\u001b[39m=\u001b[39;49mmask\u001b[39m.\u001b[39;49mtranspose(\u001b[39m0\u001b[39;49m, \u001b[39m1\u001b[39;49m))\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/class/tutor/kasper_ML/hw3/check_ind.ipynb#X15sZmlsZQ%3D%3D?line=58'>59</a>\u001b[0m logits \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpredictor(out[:, i, :])\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/class/tutor/kasper_ML/hw3/check_ind.ipynb#X15sZmlsZQ%3D%3D?line=59'>60</a>\u001b[0m out_logits\u001b[39m.\u001b[39mappend(logits[:, \u001b[39mNone\u001b[39;00m, :])\n",
      "File \u001b[1;32mc:\\Python39\\lib\\site-packages\\torch\\nn\\modules\\module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1106\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1107\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1108\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1109\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1110\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1111\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Python39\\lib\\site-packages\\torch\\nn\\modules\\transformer.py:252\u001b[0m, in \u001b[0;36mTransformerDecoder.forward\u001b[1;34m(self, tgt, memory, tgt_mask, memory_mask, tgt_key_padding_mask, memory_key_padding_mask)\u001b[0m\n\u001b[0;32m    249\u001b[0m output \u001b[39m=\u001b[39m tgt\n\u001b[0;32m    251\u001b[0m \u001b[39mfor\u001b[39;00m mod \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayers:\n\u001b[1;32m--> 252\u001b[0m     output \u001b[39m=\u001b[39m mod(output, memory, tgt_mask\u001b[39m=\u001b[39;49mtgt_mask,\n\u001b[0;32m    253\u001b[0m                  memory_mask\u001b[39m=\u001b[39;49mmemory_mask,\n\u001b[0;32m    254\u001b[0m                  tgt_key_padding_mask\u001b[39m=\u001b[39;49mtgt_key_padding_mask,\n\u001b[0;32m    255\u001b[0m                  memory_key_padding_mask\u001b[39m=\u001b[39;49mmemory_key_padding_mask)\n\u001b[0;32m    257\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnorm \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    258\u001b[0m     output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnorm(output)\n",
      "File \u001b[1;32mc:\\Python39\\lib\\site-packages\\torch\\nn\\modules\\module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1106\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1107\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1108\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1109\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1110\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1111\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Python39\\lib\\site-packages\\torch\\nn\\modules\\transformer.py:457\u001b[0m, in \u001b[0;36mTransformerDecoderLayer.forward\u001b[1;34m(self, tgt, memory, tgt_mask, memory_mask, tgt_key_padding_mask, memory_key_padding_mask)\u001b[0m\n\u001b[0;32m    455\u001b[0m     x \u001b[39m=\u001b[39m x \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_ff_block(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnorm3(x))\n\u001b[0;32m    456\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 457\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnorm1(x \u001b[39m+\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sa_block(x, tgt_mask, tgt_key_padding_mask))\n\u001b[0;32m    458\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnorm2(x \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_mha_block(x, memory, memory_mask, memory_key_padding_mask))\n\u001b[0;32m    459\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnorm3(x \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_ff_block(x))\n",
      "File \u001b[1;32mc:\\Python39\\lib\\site-packages\\torch\\nn\\modules\\transformer.py:466\u001b[0m, in \u001b[0;36mTransformerDecoderLayer._sa_block\u001b[1;34m(self, x, attn_mask, key_padding_mask)\u001b[0m\n\u001b[0;32m    464\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_sa_block\u001b[39m(\u001b[39mself\u001b[39m, x: Tensor,\n\u001b[0;32m    465\u001b[0m               attn_mask: Optional[Tensor], key_padding_mask: Optional[Tensor]) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m--> 466\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mself_attn(x, x, x,\n\u001b[0;32m    467\u001b[0m                        attn_mask\u001b[39m=\u001b[39;49mattn_mask,\n\u001b[0;32m    468\u001b[0m                        key_padding_mask\u001b[39m=\u001b[39;49mkey_padding_mask,\n\u001b[0;32m    469\u001b[0m                        need_weights\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)[\u001b[39m0\u001b[39m]\n\u001b[0;32m    470\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout1(x)\n",
      "File \u001b[1;32mc:\\Python39\\lib\\site-packages\\torch\\nn\\modules\\module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1106\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1107\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1108\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1109\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1110\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1111\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Python39\\lib\\site-packages\\torch\\nn\\modules\\activation.py:1038\u001b[0m, in \u001b[0;36mMultiheadAttention.forward\u001b[1;34m(self, query, key, value, key_padding_mask, need_weights, attn_mask, average_attn_weights)\u001b[0m\n\u001b[0;32m   1027\u001b[0m     attn_output, attn_output_weights \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39mmulti_head_attention_forward(\n\u001b[0;32m   1028\u001b[0m         query, key, value, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39membed_dim, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_heads,\n\u001b[0;32m   1029\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39min_proj_weight, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39min_proj_bias,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1035\u001b[0m         q_proj_weight\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mq_proj_weight, k_proj_weight\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mk_proj_weight,\n\u001b[0;32m   1036\u001b[0m         v_proj_weight\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mv_proj_weight, average_attn_weights\u001b[39m=\u001b[39maverage_attn_weights)\n\u001b[0;32m   1037\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1038\u001b[0m     attn_output, attn_output_weights \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39;49mmulti_head_attention_forward(\n\u001b[0;32m   1039\u001b[0m         query, key, value, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49membed_dim, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnum_heads,\n\u001b[0;32m   1040\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49min_proj_weight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49min_proj_bias,\n\u001b[0;32m   1041\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias_k, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias_v, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49madd_zero_attn,\n\u001b[0;32m   1042\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdropout, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mout_proj\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mout_proj\u001b[39m.\u001b[39;49mbias,\n\u001b[0;32m   1043\u001b[0m         training\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtraining,\n\u001b[0;32m   1044\u001b[0m         key_padding_mask\u001b[39m=\u001b[39;49mkey_padding_mask, need_weights\u001b[39m=\u001b[39;49mneed_weights,\n\u001b[0;32m   1045\u001b[0m         attn_mask\u001b[39m=\u001b[39;49mattn_mask, average_attn_weights\u001b[39m=\u001b[39;49maverage_attn_weights)\n\u001b[0;32m   1046\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbatch_first \u001b[39mand\u001b[39;00m is_batched:\n\u001b[0;32m   1047\u001b[0m     \u001b[39mreturn\u001b[39;00m attn_output\u001b[39m.\u001b[39mtranspose(\u001b[39m1\u001b[39m, \u001b[39m0\u001b[39m), attn_output_weights\n",
      "File \u001b[1;32mc:\\Python39\\lib\\site-packages\\torch\\nn\\functional.py:5268\u001b[0m, in \u001b[0;36mmulti_head_attention_forward\u001b[1;34m(query, key, value, embed_dim_to_check, num_heads, in_proj_weight, in_proj_bias, bias_k, bias_v, add_zero_attn, dropout_p, out_proj_weight, out_proj_bias, training, key_padding_mask, need_weights, attn_mask, use_separate_proj_weight, q_proj_weight, k_proj_weight, v_proj_weight, static_k, static_v, average_attn_weights)\u001b[0m\n\u001b[0;32m   5266\u001b[0m     correct_2d_size \u001b[39m=\u001b[39m (tgt_len, src_len)\n\u001b[0;32m   5267\u001b[0m     \u001b[39mif\u001b[39;00m attn_mask\u001b[39m.\u001b[39mshape \u001b[39m!=\u001b[39m correct_2d_size:\n\u001b[1;32m-> 5268\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mThe shape of the 2D attn_mask is \u001b[39m\u001b[39m{\u001b[39;00mattn_mask\u001b[39m.\u001b[39mshape\u001b[39m}\u001b[39;00m\u001b[39m, but should be \u001b[39m\u001b[39m{\u001b[39;00mcorrect_2d_size\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m   5269\u001b[0m     attn_mask \u001b[39m=\u001b[39m attn_mask\u001b[39m.\u001b[39munsqueeze(\u001b[39m0\u001b[39m)\n\u001b[0;32m   5270\u001b[0m \u001b[39melif\u001b[39;00m attn_mask\u001b[39m.\u001b[39mdim() \u001b[39m==\u001b[39m \u001b[39m3\u001b[39m:\n",
      "\u001b[1;31mRuntimeError\u001b[0m: The shape of the 2D attn_mask is torch.Size([256, 1]), but should be (256, 256)."
     ]
    }
   ],
   "source": [
    "for i in range(50):\n",
    "    print(f\"epoch {i}: loss: {train(i)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0\t1 0\t2 0\t3 0\t4 0\t5 0\t6 0\t7 0\t8 0\t9 0\t10 0\t11 0\t12 0\t13 0\t14 0\t15 0\t\n",
      "16 0\t17 72\t18 0\t19 0\t20 0\t21 0\t22 0\t23 0\t24 47\t25 0\t26 0\t27 0\t28 0\t29 0\t30 0\t31 0\t\n",
      "32 0\t33 0\t34 0\t35 0\t36 0\t37 0\t38 0\t39 0\t40 0\t41 0\t42 0\t43 0\t44 0\t45 0\t46 0\t47 0\t\n",
      "48 0\t49 0\t50 0\t51 0\t52 0\t53 0\t54 0\t55 0\t56 0\t57 0\t58 0\t59 0\t60 2885\t61 0\t62 407961\t63 0\t\n",
      "64 0\t65 0\t66 0\t67 0\t68 0\t69 0\t70 0\t71 0\t72 0\t73 0\t74 0\t75 0\t76 659550\t77 0\t78 0\t79 0\t\n",
      "80 0\t81 0\t82 0\t83 0\t84 0\t85 0\t86 0\t87 0\t88 0\t89 0\t90 0\t91 0\t92 0\t93 0\t94 0\t95 0\t\n",
      "96 0\t97 0\t98 0\t99 0\t100 0\t101 0\t102 0\t103 0\t104 0\t105 0\t106 0\t107 0\t108 0\t109 0\t110 0\t111 0\t\n",
      "112 0\t113 0\t114 0\t115 0\t116 0\t117 0\t118 0\t119 0\t120 0\t121 0\t122 0\t123 0\t124 0\t125 5391\t126 0\t127 0\t\n",
      "128 0\t129 0\t130 0\t131 0\t132 0\t133 0\t134 0\t135 0\t136 0\t137 0\t138 468018\t139 0\t140 0\t141 0\t142 0\t143 0\t\n",
      "144 0\t145 0\t146 0\t147 0\t148 0\t149 0\t150 0\t151 0\t152 0\t153 0\t154 0\t155 0\t156 0\t157 0\t158 0\t159 0\t\n",
      "160 0\t161 0\t162 0\t163 25\t164 0\t165 0\t166 0\t167 0\t168 0\t169 0\t170 0\t171 0\t172 0\t173 0\t174 0\t175 0\t\n",
      "176 0\t177 0\t178 0\t179 0\t180 0\t181 0\t182 0\t183 0\t184 0\t185 0\t186 0\t187 0\t188 0\t189 0\t190 0\t191 0\t\n",
      "192 0\t193 0\t194 586850\t195 0\t196 0\t197 0\t198 0\t199 0\t200 0\t201 0\t202 0\t203 0\t204 0\t205 0\t206 0\t207 0\t\n",
      "208 0\t209 0\t210 0\t211 0\t212 0\t213 0\t214 0\t215 0\t216 0\t217 0\t218 0\t219 0\t220 0\t221 0\t222 0\t223 0\t\n",
      "224 0\t225 0\t226 0\t227 629365\t228 0\t229 0\t230 0\t231 0\t232 0\t233 0\t234 0\t235 0\t236 0\t237 0\t238 0\t239 0\t\n",
      "240 0\t241 0\t242 0\t243 0\t244 0\t245 0\t246 0\t247 0\t248 0\t249 0\t250 0\t251 536\t252 4\t"
     ]
    }
   ],
   "source": [
    "\n",
    "binc = torch.bincount(ind.view([-1]))\n",
    "for i, c in enumerate(binc):\n",
    "    print(i, c.item(), end='\\t')\n",
    "    if i % 16 == 15:\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 62,  76,  62,  ..., 138,  76,  76],\n",
      "        [ 62,  76,  76,  ..., 138,  76, 138],\n",
      "        [ 62,  62,  76,  ...,  76, 138, 138],\n",
      "        ...,\n",
      "        [ 76,  76,  76,  ...,  76,  76, 138],\n",
      "        [ 62,  76,  76,  ...,  76, 227,  76],\n",
      "        [ 76,  76,  76,  ...,  76,  62, 138]], device='cuda:0')\n",
      "torch.Size([32, 256])\n",
      "torch.Size([32, 1, 256, 256])\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    \n",
    "    # emb = rearrange(emb, 'N W H E -> N ( W H ) E')\n",
    "    # cur = torch.zeros(32, 16 * 16, 8).to(DEVICE)\n",
    "    # cur = net.quantize.embedding(ind[:32])\n",
    "    # cur[:, 32:, :] = 0\n",
    "    # ans = net.quantize.embedding(ind[:32])\n",
    "    # ans = pixelCNN.positional_encoding(ans)\n",
    "        \n",
    "    # print(ind[0])\n",
    "    N, S, E = 32, 16*16, 8\n",
    "    tgt_mask = nn.Transformer.generate_square_subsequent_mask(S).cuda()\n",
    "    from einops import repeat, rearrange\n",
    "    \n",
    "    cur = torch.zeros([N, S, E], device=DEVICE)\n",
    "    out_embs = torch.zeros([N, S, E], device=DEVICE)\n",
    "    out_inds = []\n",
    "    \n",
    "    for i in range(256):\n",
    "        # print(cur[0, :,0])\n",
    "        pos_cur = pixelCNN.positional_encoding(cur)\n",
    "        out = pixelCNN.transformer(pos_cur, out_embs, tgt_mask=tgt_mask)\n",
    "        out_ind = torch.argmax(pixelCNN.predictor(out[:, i, :]), dim=1)\n",
    "        out_inds.append(out_ind[:, None])\n",
    "        out_emb = net.quantize.embedding(out_ind)\n",
    "        cur[:, i, :] = out_emb\n",
    "        out_embs[:, i, :] = out_emb\n",
    "        # print(out_emb[:, None, :].shape)\n",
    "        # cur [:, i, :] = \n",
    "    # nxt_ind = torch.argmax(pixelCNN(net.quantize.embedding(ind[:32])), dim=2)\n",
    "    out_inds = torch.cat(out_inds, dim=1)\n",
    "    print(out_inds)\n",
    "    print(out_inds.shape)\n",
    "    z_q = net.quantize.embedding(out_inds)\n",
    "    z_q = rearrange(z_q, 'b (w h) c -> b c w h', w = 16)\n",
    "    sample = net.decode(z_q).cpu()\n",
    "\n",
    "    print(sample.shape)\n",
    "    from util import compact_large_image\n",
    "    imgs = compact_large_image(sample, HZ=4, WZ=8)\n",
    "    for idx in range(imgs.shape[0]):\n",
    "        plt.imsave('test.png', imgs[0] * 0.5 + 0.5, cmap='gray')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10784, 256])\n",
      "{17: 0, 24: 1, 60: 2, 62: 3, 76: 4, 125: 5, 138: 6, 163: 7, 194: 8, 227: 9, 251: 10, 252: 11}\n",
      "[[8 6 6]\n",
      " [8 9 4]\n",
      " [8 9 4]\n",
      " ...\n",
      " [8 9 4]\n",
      " [8 9 4]\n",
      " [8 4 4]]\n"
     ]
    }
   ],
   "source": [
    "print(ind.shape)\n",
    "map_id = { x.item(): i for i, x in enumerate(torch.unique(ind.view([-1])))}\n",
    "print(map_id)\n",
    "remap_id = { i: x for x, i in map_id.items()}\n",
    "ind2 = ind.cpu().numpy()\n",
    "for i, x in map_id.items():\n",
    "    ind2[ind2 == i] = x\n",
    "print(ind2[:, :3])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
