# -*- coding: utf-8 -*-
"""train.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1dXHq1cG6htmVWaxo_l9Cjr9LFVt96P2L
"""

# Commented out IPython magic to ensure Python compatibility.
'''
Simple

'''
from google.colab import drive
drive.mount('/content/drive')

# %cd /content/drive/MyDrive/ColabNotebooks

from datasetv2 import MoleData
from modulesv2 import load_model
import torch
import torch.nn as nn
import matplotlib.pyplot as plt
import time
from tqdm import tqdm

MODE = "debug"
device = torch.device("cuda") if torch.cuda.is_available() else torch.device("cpu")
model = load_model()
model = model.to(device)
if MODE == "debug":
    print(device)

#Hyper Parameters
num_epochs = 5
learning_rate = 0.0025
num_classes = 3

#Train Data
train_data = MoleData("/content/drive/MyDrive/ColabNotebooks/ISIC-2017-DATA/ISIC-2017_Training_Data",
                 "/content/drive/MyDrive/ColabNotebooks/ISIC-2017-DATA/ISIC-2017_Training_Part1_GroundTruth",
                 "/content/drive/MyDrive/ColabNotebooks/ISIC-2017-DATA/ISIC-2017_Training_Part3_GroundTruth.csv")

train_data = torch.utils.data.Subset(train_data, range(100))
train_dataloader = torch.utils.data.DataLoader( train_data, batch_size=1, shuffle=True, collate_fn=lambda x:tuple(zip(*x)))

## Test Data
test_data = MoleData("//content/drive/MyDrive/ColabNotebooks/ISIC-2017-DATA/ISIC-2017_Test_v2_Data",
                 "/content/drive/MyDrive/ColabNotebooks/ISIC-2017-DATA/ISIC-2017_Test_v2_Part1_GroundTruth",
                 "/content/drive/MyDrive/ColabNotebooks/ISIC-2017-DATA/ISIC-2017_Test_v2_Part3_GroundTruth.csv")

#batch_size = 256
test_data = torch.utils.data.Subset(test_data, range(100))
test_dataloader = torch.utils.data.DataLoader(test_data, batch_size=1, shuffle=True, collate_fn=lambda x:tuple(zip(*x)))

def single_epoch(model, optimizer, dataloader, device, epoch):
  total_losses = []
  i = 0
  total_step = len(dataloader)
  for images,targets in tqdm(dataloader): #load a batch
          images = [image.to(device) for image in images]
          targets = [{k: v.to(device) for k, v in target.items()} for target in targets]

          # Forward pass
          optimizer.zero_grad()
          outputs = model(images,targets)
          total_loss = sum(loss for loss in outputs.values())

          # Backward and optimize

          total_loss.backward()
          optimizer.step()
          total_losses.append(total_loss.detach().cpu().item())
          i += 1
          if (i+1) % 100 == 0:
              print ("Epoch [{}/{}], Step [{}/{}] Loss: {:.5f}"
                      .format(i+1, num_epochs, i+1, total_step, total_loss.item()))


  return total_losses


def train_model():
  # criterion = nn.CrossEntropyLoss()
  optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate,weight_decay=0.0001, momentum=0.9)

  #linear schedule - how to reduce learning rate as training
  total_step = len(train_dataloader)
  scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=learning_rate, steps_per_epoch=total_step, epochs=num_epochs)
  #scheduler =  torch.optim.lr_scheduler.StepLR(optimizer,step_size=3,gamma=0.1)
  total_loss = []

  #Train
  model.train()
  training_loss = []
  testing_loss = []
  for epoch in range(num_epochs):
    print(">Training")
    start = time.time()
    all_loss = single_epoch(model, optimizer, train_dataloader, device, epoch)
    scheduler.step()
    training_loss.append(sum(all_loss))
    print("TRAINING_LOSS", sum(all_loss))
    end = time.time()
    elapsed = end - start
    print("Training took " + str(elapsed) + " secs or " + str(elapsed/60) + " mins in total")

    #Testing
    print("> Testing")
    start = time.time() #time generation
    with torch.no_grad():
        correct = 0
        total = 0
        all_losses = []
        for images,targets in tqdm(test_dataloader): #load a batch
          images = [image.to(device) for image in images]
          targets = [{k: v.to(device) for k, v in target.items()} for target in targets]
          outputs = model(images,targets)
          total_loss = sum(loss for loss in outputs.values())
          all_losses.append(total_loss.detach().cpu().item())

        testing_loss.append(sum(all_losses))
        print("TESTING_LOSS", sum(all_losses))

  end = time.time()
  elapsed = end - start
  print("Testing took " + str(elapsed) + " secs or " + str(elapsed/60) + " mins in total")
  print('END')
  fig, ax = plt.subplots()
  ax.plot(training_loss, label="Train")
  ax.plot(testing_loss, label="Test")
  ax.legend()

  return training_loss, testing_loss


training_loss, testing_loss= train_model()

torch.save(model.state_dict(), "Mask_RCNN_ISIC4try2.pt")

training_loss2, testing_loss2= train_model()
training_loss3, testing_loss3= train_model()