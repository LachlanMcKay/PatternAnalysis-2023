{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "6230ed1d-f225-4933-aeb1-651eacdd3e61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda:0\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "#import modules\n",
    "import argparse\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "from tkinter.tix import IMAGE\n",
    "import dataset\n",
    "import modules\n",
    "import train\n",
    "import predict\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.nn.parallel\n",
    "import torch.optim as optim\n",
    "import torch.utils.data\n",
    "import torchvision\n",
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import Dataset\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision.transforms.functional as TF\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from glob import glob\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "# Device\n",
    "CUDA_DEVICE_NUM = 0\n",
    "DEVICE = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print('Device:', DEVICE)\n",
    "print(torch.cuda.is_available())\n",
    "\n",
    "# Hyperparameters\n",
    "LEARNING_RATE = 0.0001\n",
    "ROOTLOC = \"\\ISIC\"\n",
    "TRAINDATA = \"ISIC\\ISIC-2017_Training_Data\\ISIC-2017_Training_Data\"\n",
    "TESTDATA = \"ISIC\\ISIC-2017_Test_v2_Data\\ISIC-2017_Test_v2_Data\"\n",
    "VALIDDATA = \"ISIC\\ISIC-2017_Validation_Data\\ISIC-2017_Validation_Data\"\n",
    "TRAINTRUTH = \"ISIC\\ISIC-2017_Training_Part1_GroundTruth\\ISIC-2017_Training_Part1_GroundTruth\"\n",
    "TESTTRUTH = \"ISIC\\ISIC-2017_Test_v2_Part1_GroundTruth\\ISIC-2017_Test_v2_Part1_GroundTruth\"\n",
    "VALIDTRUTH = \"ISIC\\ISIC-2017_Validation_Part1_GroundTruth\\ISIC-2017_Validation_Part1_GroundTruth\"\n",
    "\n",
    "NUM_EPOCHS = 5\n",
    "BATCH_SIZE = 4\n",
    "WORKERS = 4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "f1cf9970",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<enumerate object at 0x00000217B939FD80>\n",
      "1001\n",
      "301\n",
      "4001\n",
      "1201\n"
     ]
    }
   ],
   "source": [
    "\n",
    "train_dataset = dataset.CustomDataset(image_dir = TRAINDATA,\n",
    "                                mask_dir=TRAINTRUTH,\n",
    "                                transform=transforms.Compose([\n",
    "                                transforms.RandomRotation(30),\n",
    "                                transforms.RandomResizedCrop(224),\n",
    "                                transforms.RandomHorizontalFlip(),\n",
    "                                transforms.ToTensor()]))\n",
    "\n",
    "train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=BATCH_SIZE,\n",
    "                                         shuffle=True)\n",
    "\n",
    "valid_dataset = dataset.CustomDataset(image_dir = VALIDDATA,\n",
    "                                mask_dir=VALIDTRUTH,\n",
    "                                transform=transforms.Compose([\n",
    "                                transforms.RandomRotation(30),\n",
    "                                transforms.RandomResizedCrop(224),\n",
    "                                transforms.RandomHorizontalFlip(),\n",
    "                                transforms.ToTensor()]))\n",
    "\n",
    "valid_dataloader = torch.utils.data.DataLoader(valid_dataset, batch_size=BATCH_SIZE,\n",
    "                                         shuffle=True)\n",
    "\n",
    "test_dataset = dataset.CustomDataset(image_dir = TESTDATA,\n",
    "                                mask_dir=TESTTRUTH,\n",
    "                                transform=transforms.Compose([\n",
    "                                transforms.RandomRotation(30),\n",
    "                                transforms.RandomResizedCrop(224),\n",
    "                                transforms.RandomHorizontalFlip(),\n",
    "                                transforms.ToTensor()]))\n",
    "\n",
    "test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=BATCH_SIZE,\n",
    "                                         shuffle=True)\n",
    "\n",
    "\n",
    "#print(train_dataset.__getitem__(1))\n",
    "#print(train_dataset.__getitem__(1)[0].shape)\n",
    "#print(train_dataset.__getitem__(1)[1].shape)\n",
    "print(enumerate(train_dataloader))\n",
    "print(len(train_dataloader))\n",
    "print(len(test_dataloader))\n",
    "print(len(train_dataset))\n",
    "print(len(test_dataset))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "f2e48156",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for i, sample in enumerate(train_dataset):\n",
    "    #print(sample)\n",
    "    image = sample['image']\n",
    "    mask = sample['mask']\n",
    "\n",
    "    #print(i, sample['image'].shape, sample['mask'].shape)\n",
    "\n",
    "    ax = plt.subplot(1, 4, i + 1)\n",
    "    plt.tight_layout()\n",
    "    ax.set_title('Sample #{}'.format(i))\n",
    "    ax.axis('off')\n",
    "    #image = image.numpy()\n",
    "    image = np.array(image)\n",
    "    #print(\"image:\", image.shape)\n",
    "    #print(type(image))\n",
    "    image = np.transpose(image, (1, 2, 0))\n",
    "\n",
    "    plt.imshow(image)\n",
    "    plt.show()\n",
    "    if i == 3:\n",
    "        plt.show()\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a734099",
   "metadata": {},
   "source": [
    "TRAIN "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "3160330f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "1001\n"
     ]
    }
   ],
   "source": [
    "model = train.UNet(3,1,[64,128,256,512]) \n",
    "model = model.to(DEVICE)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=LEARNING_RATE, \n",
    "                            momentum = 0.9, weight_decay = 5e-4)\n",
    "\n",
    "total_steps = len(train_dataloader)\n",
    "#scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer,max_lr = LEARNING_RATE, \n",
    "#                                steps_per_epoch = total_steps, epochs = NUM_EPOCHS)\n",
    "print(total_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "27f1cf1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Training\n",
      "<enumerate object at 0x00000217249A06D0>\n",
      "<torch.utils.data.dataloader.DataLoader object at 0x00000217D87665D0>\n"
     ]
    }
   ],
   "source": [
    "model.train()\n",
    "print(\"> Training\")\n",
    "print(enumerate(train_dataloader))\n",
    "print(train_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "89768b9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_predictions_as_imgs(loader, model, folder=\"saved_images/\", device=\"cuda\"):\n",
    "    model.eval()\n",
    "    for idx, batch in enumerate(loader):\n",
    "        images = batch['image']\n",
    "        masks = batch['mask']\n",
    "        x = images.to(device=DEVICE)\n",
    "        masks = masks.to(DEVICE)\n",
    "        with torch.no_grad():\n",
    "            preds = torch.sigmoid(model(x))\n",
    "            preds = (preds > 0.5).float()\n",
    "        torchvision.utils.save_image(\n",
    "            preds, f\"{folder}/pred_{idx}.png\"\n",
    "        )\n",
    "        torchvision.utils.save_image(masks, f\"{folder}{idx}.png\")\n",
    "\n",
    "    model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "e4caf371",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 0\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32md:\\8.1.COMPSC_WORK\\COMP3710_DEMO3\\PatternAnalysis-2023\\recognition\\JunHyukKim_DEMO3\\ISIC_DEMO3.ipynb Cell 8\u001b[0m line \u001b[0;36m3\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/8.1.COMPSC_WORK/COMP3710_DEMO3/PatternAnalysis-2023/recognition/JunHyukKim_DEMO3/ISIC_DEMO3.ipynb#X14sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(NUM_EPOCHS):\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/8.1.COMPSC_WORK/COMP3710_DEMO3/PatternAnalysis-2023/recognition/JunHyukKim_DEMO3/ISIC_DEMO3.ipynb#X14sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mEPOCH:\u001b[39m\u001b[39m\"\u001b[39m,epoch)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/8.1.COMPSC_WORK/COMP3710_DEMO3/PatternAnalysis-2023/recognition/JunHyukKim_DEMO3/ISIC_DEMO3.ipynb#X14sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     \u001b[39mfor\u001b[39;00m i, batch \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(train_dataloader):\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/8.1.COMPSC_WORK/COMP3710_DEMO3/PatternAnalysis-2023/recognition/JunHyukKim_DEMO3/ISIC_DEMO3.ipynb#X14sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m         images \u001b[39m=\u001b[39m batch[\u001b[39m'\u001b[39m\u001b[39mimage\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/8.1.COMPSC_WORK/COMP3710_DEMO3/PatternAnalysis-2023/recognition/JunHyukKim_DEMO3/ISIC_DEMO3.ipynb#X14sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m         masks \u001b[39m=\u001b[39m batch[\u001b[39m'\u001b[39m\u001b[39mmask\u001b[39m\u001b[39m'\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\John\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:633\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    630\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    631\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    632\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 633\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[0;32m    634\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m    635\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m    636\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m    637\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32mc:\\Users\\John\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:677\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    675\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m    676\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 677\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset_fetcher\u001b[39m.\u001b[39;49mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    678\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[0;32m    679\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32mc:\\Users\\John\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     49\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     50\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m---> 51\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataset[idx] \u001b[39mfor\u001b[39;49;00m idx \u001b[39min\u001b[39;49;00m possibly_batched_index]\n\u001b[0;32m     52\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32mc:\\Users\\John\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     49\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     50\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m---> 51\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     52\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32md:\\8.1.COMPSC_WORK\\COMP3710_DEMO3\\PatternAnalysis-2023\\recognition\\JunHyukKim_DEMO3\\dataset.py:22\u001b[0m, in \u001b[0;36m__getitem__\u001b[1;34m(self, index)\u001b[0m\n\u001b[0;32m     19\u001b[0m VALIDTRUTH \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mISIC\u001b[39m\u001b[39m\\\u001b[39m\u001b[39mISIC-2017_Validation_Part1_GroundTruth\u001b[39m\u001b[39m\\\u001b[39m\u001b[39mISIC-2017_Validation_Part1_GroundTruth\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m     21\u001b[0m NUM_EPOCHS \u001b[39m=\u001b[39m \u001b[39m5\u001b[39m\n\u001b[1;32m---> 22\u001b[0m BATCH_SIZE \u001b[39m=\u001b[39m \u001b[39m4\u001b[39m\n\u001b[0;32m     23\u001b[0m WORKERS \u001b[39m=\u001b[39m \u001b[39m4\u001b[39m\n\u001b[0;32m     25\u001b[0m \u001b[39mclass\u001b[39;00m \u001b[39mCustomDataset\u001b[39;00m(Dataset):\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "for epoch in range(NUM_EPOCHS):\n",
    "    print(\"EPOCH:\",epoch)\n",
    "    for i, batch in enumerate(train_dataloader):\n",
    "        images = batch['image']\n",
    "        masks = batch['mask']\n",
    "        #print(images.shape)\n",
    "        images = images.to(DEVICE)\n",
    "        masks = masks.to(DEVICE)\n",
    "        #print(images.shape)\n",
    "        outputs = model(images)\n",
    "        #print(1,outputs)\n",
    "        #print(2,masks)\n",
    "        loss = criterion(outputs, masks)\n",
    "        #print(i)\n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if (i+1) % 100 == 0:\n",
    "            print (f'Epoch [{epoch+1}/{NUM_EPOCHS}], Step [{i+1}/{total_steps}], Loss: {loss.item():.5f}')\n",
    "        #scheduler.step()\n",
    "            modules.save_predictions_as_imgs(train_dataloader,model)\n",
    "print(\"Training took \" +  \" secs\")\n",
    "print(\"> Testing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f35d333",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
