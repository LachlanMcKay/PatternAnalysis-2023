{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch Version: 2.0.1\n",
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torch.utils.data import DataLoader, ConcatDataset\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import time\n",
    "import sys\n",
    "\n",
    "print(\"PyTorch Version:\", torch.__version__)\n",
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(\"Using device:\", device)\n",
    "sys.stdout.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters and config\n",
    "# Path to image folders - use both AD and NC when training\n",
    "# AD = Alzheimerâ€™s disease\n",
    "# CN = Cognitive Normal\n",
    "AD_dir = \"data/AD_NC/train/AD-parent/\"\n",
    "NC_dir = \"data/AD_NC/train/NC-parent/\"\n",
    "\n",
    "image_dir = \"imgs/\"\n",
    "\n",
    "batch_size = 128\n",
    "\n",
    "num_epochs = 10\n",
    "\n",
    "# Factor to reduce the width and height of the images by\n",
    "dimension_reduce_factor = 2  # downsample factor of 4\n",
    "\n",
    "# Original dimensions\n",
    "original_width = 256\n",
    "original_height = 240"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def downsample_tensor(original: torch.Tensor):\n",
    "    return transforms.Resize([\n",
    "        original_height // dimension_reduce_factor, \n",
    "        original_width // dimension_reduce_factor\n",
    "    ],antialias=True)(original)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original.shape torch.Size([128, 3, 240, 256])\n",
      "batch inputs shape: torch.Size([128, 3, 120, 128])\n"
     ]
    }
   ],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "AD_dataset = ImageFolder(root=AD_dir, transform=transform)\n",
    "NC_dataset = ImageFolder(root=NC_dir, transform=transform)\n",
    "\n",
    "dataset = ConcatDataset([AD_dataset, NC_dataset])\n",
    "\n",
    "# Create a data loader to iterate through the dataset\n",
    "data_loader = DataLoader(dataset=dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "for original, _ in data_loader:\n",
    "    print('original.shape', original.shape)\n",
    "\n",
    "    # Look at the first image\n",
    "    output = original[0]\n",
    "    # Downsample to get input\n",
    "    input = downsample_tensor(output)\n",
    "\n",
    "    all_inputs = downsample_tensor(original)\n",
    "    print('batch inputs shape:', all_inputs.shape)\n",
    "    \n",
    "    input_dims = input.shape\n",
    "    output_dims = output.shape\n",
    "    \n",
    "    # Display the input image\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.imshow(input.permute(1, 2, 0))  # Convert tensor to numpy format (C, H, W) -> (H, W, C)\n",
    "    plt.title(f\"Input Image Dimensions: {input_dims}\")\n",
    "    plt.axis('off')  # Turn off axis labels\n",
    "    \n",
    "    # Display the first output image\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.imshow(torch.clamp(output.permute(1, 2, 0), 0, 1))  # Convert tensor to numpy format (C, H, W) -> (H, W, C)\n",
    "    plt.title(f\"Output Image Dimensions: {output_dims}\")\n",
    "    plt.axis('off') # Turn off axis labels\n",
    "    \n",
    "    plt.savefig(image_dir + 'dimensions.png')\n",
    "    # plt.show()\n",
    "    plt.close()\n",
    "    \n",
    "    break  # Stop after the first batch to print/display only the first pair of images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SuperResolutionModel(nn.Module):\n",
    "    def __init__(self, upscale_factor=2, channels=3):\n",
    "        super(SuperResolutionModel, self).__init__()\n",
    "        \n",
    "        self.inputs = nn.Sequential(\n",
    "            nn.Conv2d(channels, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(64, 32, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(32, channels * (upscale_factor ** 2), kernel_size=3, padding=1),\n",
    "        )\n",
    "        \n",
    "        self.outputs = nn.Sequential(\n",
    "            nn.PixelShuffle(upscale_factor),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.inputs(x)\n",
    "        x = self.outputs(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an instance of the super-resolution model\n",
    "model = SuperResolutionModel(upscale_factor=dimension_reduce_factor)\n",
    "\n",
    "# Test the model with an example input\n",
    "input_tensor = torch.randn(1, 3, original_height // dimension_reduce_factor, \n",
    "        original_width // dimension_reduce_factor)  # Batch size of 1\n",
    "output_tensor = model(input_tensor)\n",
    "\n",
    "# Check the shape of the output\n",
    "print(\"Input shape:\", input_tensor.shape)\n",
    "print(\"Output shape:\", output_tensor.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model_output(prefix: str):\n",
    "    with torch.no_grad():\n",
    "        for expected_outputs, _ in data_loader:\n",
    "                inputs = downsample_tensor(expected_outputs)\n",
    "                \n",
    "                # Get the dimensions of the first input image\n",
    "                first_input_image = inputs[0]\n",
    "                \n",
    "                # Get the dimensions of the first output image\n",
    "                first_output_image = expected_outputs[0]\n",
    "\n",
    "                # Actual model output\n",
    "                first_model_output = model(first_input_image)\n",
    "                # Scale output to be in correct RGB range\n",
    "                first_model_output = torch.clamp(first_model_output, 0, 1.0)\n",
    "                \n",
    "                # Display the first input image\n",
    "                plt.figure(figsize=(18, 6))\n",
    "                plt.subplot(1, 3, 1) # first column\n",
    "                # Convert tensor to numpy format (C, H, W) -> (H, W, C)\n",
    "                input_img_formatted = first_input_image.permute(1, 2, 0)\n",
    "                plt.imshow(input_img_formatted)\n",
    "                plt.title(\"Input Image\")\n",
    "                plt.axis('off')  # Turn off axis labels\n",
    "                \n",
    "                # Display the first output image\n",
    "                plt.subplot(1, 3, 3) # third column\n",
    "                # Convert tensor to numpy format (C, H, W) -> (H, W, C)\n",
    "                output_img_formatted = first_output_image.permute(1, 2, 0)\n",
    "                plt.imshow(output_img_formatted)\n",
    "                plt.title(\"Target Image\")\n",
    "                plt.axis('off')  # Turn off axis labels\n",
    "\n",
    "                # Display the first model output\n",
    "                plt.subplot(1, 3, 2) # second column\n",
    "                # Convert tensor to numpy format (C, H, W) -> (H, W, C)\n",
    "                output_img_formatted = first_model_output.permute(1, 2, 0)\n",
    "                output_img_formatted = torch.clamp(output_img_formatted, 0, 1)\n",
    "                plt.imshow(output_img_formatted)\n",
    "                plt.title(f\"Model Output\")\n",
    "                plt.axis('off')  # Turn off axis labels\n",
    "                \n",
    "                filename = image_dir + prefix + 'output.png'\n",
    "\n",
    "                plt.savefig(filename)\n",
    "                print(\"Saved model output to\", filename)\n",
    "                \n",
    "                break  # Stop after the first batch to print/display only the first pair of images\n",
    "save_model_output('test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the loss function (MSE) and optimizer (Adam)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "start = time.time()\n",
    "print(\"Starting training...\")\n",
    "sys.stdout.flush()\n",
    "\n",
    "n = len(data_loader)\n",
    "\n",
    "save_model_output(f\"[{1},{num_epochs}][{0},{n}]\")\n",
    "\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    running_loss = 0.0\n",
    "\n",
    "    i = 0\n",
    "    \n",
    "    for expected_outputs, _ in data_loader:\n",
    "        inputs = downsample_tensor(expected_outputs)\n",
    "        \n",
    "        # Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "\n",
    "        # Compute the loss\n",
    "        loss = criterion(outputs, expected_outputs)\n",
    "\n",
    "        # Backpropagation and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "        i += 1\n",
    "        if i % 10 == 0 or i == 1:\n",
    "            print(f\"Finished [{i},{n}] loss: {loss.item()}\")\n",
    "            sys.stdout.flush()\n",
    "        if i % 40 == 0 or (i == 1 and epoch == 0):\n",
    "            save_model_output(f\"[{epoch + 1},{num_epochs}][{i},{n}]\")\n",
    "            sys.stdout.flush()\n",
    "\n",
    "    # Print the average loss for the epoch\n",
    "    average_loss = running_loss / len(data_loader)\n",
    "    print(f\"Epoch [{epoch + 1},{num_epochs}] Loss: {average_loss:.4f}\")\n",
    "    sys.stdout.flush()\n",
    "\n",
    "end = time.time()\n",
    "print(f\"Training finished. Took {round((end - start) / 60, 1)} minutes\")\n",
    "sys.stdout.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the trained model to a file\n",
    "model_path = \"super_resolution_model.pth\"\n",
    "torch.save(model.state_dict(), model_path)\n",
    "\n",
    "print(f\"Model saved to {model_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_model_output(f\"[{num_epochs},{num_epochs}][{n},{n}]\")\n",
    "print(\"Finished!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
