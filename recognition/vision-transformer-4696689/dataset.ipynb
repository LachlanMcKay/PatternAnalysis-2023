{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d12ae03c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/oliver/opt/anaconda3/lib/python3.9/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: 'dlopen(/Users/oliver/opt/anaconda3/lib/python3.9/site-packages/torchvision/image.so, 6): Library not loaded: @rpath/libpng16.16.dylib\n",
      "  Referenced from: /Users/oliver/opt/anaconda3/lib/python3.9/site-packages/torchvision/image.so\n",
      "  Reason: Incompatible library version: image.so requires version 56.0.0 or later, but libpng16.16.dylib provides version 54.0.0'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
      "  warn(\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Imports Here\n",
    "\"\"\"\n",
    "\"\"\"numpy and torch\"\"\"\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "\"\"\"PIL\"\"\"\n",
    "from PIL import Image\n",
    "\n",
    "\"\"\"torchvision and utils\"\"\"\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "\"\"\"os\"\"\"\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d7621a39",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nLoading data from local file\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Loading data from local file\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "206e485b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Assumes images have pixel values in range [0,255]\"\"\"\n",
    "def getImages(trainDIRs, testDIRS):\n",
    "    \"\"\"Get image to tensor\"\"\"\n",
    "    transform = transforms.Compose([\n",
    "        transforms.PILToTensor()\n",
    "    ])\n",
    "    \"\"\"Loading data into arrays\"\"\"\n",
    "    xtrain, xtrain, xtest, ytest = [], [], [], []\n",
    "    \"\"\"training data\"\"\"\n",
    "    size = [0, 0]\n",
    "    for i, DIR in enumerate(trainDIRs):\n",
    "        for filename in os.listdir(DIR):\n",
    "            f = os.path.join(DIR, filename)\n",
    "            img = Image.open(f)\n",
    "            tensor = transform(img).float()\n",
    "            tensor.require_grad = True\n",
    "            xtrain.append(tensor/255)\n",
    "            size[i] += 1\n",
    "    xtrain = torch.stack(xtrain)\n",
    "    ytrain = torch.from_numpy(np.concatenate((np.ones(size[0]), np.zeros(size[1])), axis=0))\n",
    "    \"\"\"testing data\"\"\"\n",
    "    size = [0, 0]\n",
    "    for i, DIR in enumerate(testDIRs):\n",
    "        for filename in os.listdir(DIR):\n",
    "            f = os.path.join(DIR, filename)\n",
    "            img = Image.open(f)\n",
    "            tensor = transform(img).float()\n",
    "            tensor.require_grad = True\n",
    "            xtest.append(tensor/255)\n",
    "            size[i] += 1\n",
    "    xtest = torch.stack(xtest)\n",
    "    ytest = torch.from_numpy(np.concatenate((np.ones(size[0]), np.zeros(size[1])), axis=0))\n",
    "    return xtrain, ytrain, xtest, ytest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "16828153",
   "metadata": {},
   "outputs": [],
   "source": [
    "def createPatches(imgs, patchsize):\n",
    "    (N, C, W, H) = imgs.shape\n",
    "    (wsize, hsize) = patchsize\n",
    "    \"\"\"check for errors with sizing\"\"\"\n",
    "    if (W % wsize != 0) or (H % hsize != 0):\n",
    "        raise Exception(\"patchsize is not appropriate\")\n",
    "    if (C != C) or (H != H):\n",
    "        raise Exception(\"given sizes do not match\")\n",
    "    size = (N, C, W // wsize, wsize, H // hsize, hsize)\n",
    "    perm = (0, 2, 4, 1, 3, 5) #bring col, row index of patch to front\n",
    "    flat = (1, 2) #flatten (col, row) index into col*row entry index for patches\n",
    "    imgs = imgs.reshape(size).permute(perm).flatten(*flat)\n",
    "    return imgs #in format Nimgs, Npatches, C, Wpatch, Hpatch\n",
    "    \n",
    "def flattenPatches(imgs): #takes input (N, Npatches, C, W, H)\n",
    "    return imgs.flatten(2, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8dd9ba68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nDataloader\\n'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Dataloader\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0da50adb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DatasetWrapper(Dataset):\n",
    "    def __init__(self, X, y=None):\n",
    "        self.X, self.y = X, y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if self.y is None:\n",
    "            return self.X[idx]\n",
    "        else:\n",
    "            return self.X[idx], self.y[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "00855a85",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [7]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m trainDIRs \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m../../../AD_NC/train/AD/\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m../../../AD_NC/train/NC\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m      2\u001b[0m testDIRs \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m../../../AD_NC/test/AD/\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m../../../AD_NC/test/NC\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m----> 3\u001b[0m xtrain, ytrain, xtest, ytest \u001b[38;5;241m=\u001b[39m \u001b[43mgetImages\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrainDIRs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtestDIRs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [3]\u001b[0m, in \u001b[0;36mgetImages\u001b[0;34m(trainDIRs, testDIRS)\u001b[0m\n\u001b[1;32m     13\u001b[0m f \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(DIR, filename)\n\u001b[1;32m     14\u001b[0m img \u001b[38;5;241m=\u001b[39m Image\u001b[38;5;241m.\u001b[39mopen(f)\n\u001b[0;32m---> 15\u001b[0m tensor \u001b[38;5;241m=\u001b[39m \u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mfloat()\n\u001b[1;32m     16\u001b[0m tensor\u001b[38;5;241m.\u001b[39mrequire_grad \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m     17\u001b[0m xtrain\u001b[38;5;241m.\u001b[39mappend(tensor\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m255\u001b[39m)\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torchvision/transforms/transforms.py:95\u001b[0m, in \u001b[0;36mCompose.__call__\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, img):\n\u001b[1;32m     94\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransforms:\n\u001b[0;32m---> 95\u001b[0m         img \u001b[38;5;241m=\u001b[39m \u001b[43mt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     96\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m img\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torchvision/transforms/transforms.py:166\u001b[0m, in \u001b[0;36mPILToTensor.__call__\u001b[0;34m(self, pic)\u001b[0m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, pic):\n\u001b[1;32m    155\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    156\u001b[0m \u001b[38;5;124;03m    .. note::\u001b[39;00m\n\u001b[1;32m    157\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    164\u001b[0m \u001b[38;5;124;03m        Tensor: Converted image.\u001b[39;00m\n\u001b[1;32m    165\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 166\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpil_to_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpic\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torchvision/transforms/functional.py:207\u001b[0m, in \u001b[0;36mpil_to_tensor\u001b[0;34m(pic)\u001b[0m\n\u001b[1;32m    204\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mas_tensor(nppic)\n\u001b[1;32m    206\u001b[0m \u001b[38;5;66;03m# handle PIL Image\u001b[39;00m\n\u001b[0;32m--> 207\u001b[0m img \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mas_tensor(\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpic\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m)\n\u001b[1;32m    208\u001b[0m img \u001b[38;5;241m=\u001b[39m img\u001b[38;5;241m.\u001b[39mview(pic\u001b[38;5;241m.\u001b[39msize[\u001b[38;5;241m1\u001b[39m], pic\u001b[38;5;241m.\u001b[39msize[\u001b[38;5;241m0\u001b[39m], F_pil\u001b[38;5;241m.\u001b[39mget_image_num_channels(pic))\n\u001b[1;32m    209\u001b[0m \u001b[38;5;66;03m# put it from HWC to CHW format\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/PIL/Image.py:675\u001b[0m, in \u001b[0;36mImage.__array__\u001b[0;34m(self, dtype)\u001b[0m\n\u001b[1;32m    673\u001b[0m     new[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtobytes(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraw\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mL\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    674\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 675\u001b[0m     new[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtobytes\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    677\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39marray(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ArrayData(new), dtype)\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/PIL/Image.py:718\u001b[0m, in \u001b[0;36mImage.tobytes\u001b[0;34m(self, encoder_name, *args)\u001b[0m\n\u001b[1;32m    715\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m encoder_name \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraw\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m args \u001b[38;5;241m==\u001b[39m ():\n\u001b[1;32m    716\u001b[0m     args \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmode\n\u001b[0;32m--> 718\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    720\u001b[0m \u001b[38;5;66;03m# unpack data\u001b[39;00m\n\u001b[1;32m    721\u001b[0m e \u001b[38;5;241m=\u001b[39m _getencoder(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmode, encoder_name, args)\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/PIL/ImageFile.py:253\u001b[0m, in \u001b[0;36mImageFile.load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    247\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(\n\u001b[1;32m    248\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimage file is truncated \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    249\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(b)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m bytes not processed)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    250\u001b[0m         )\n\u001b[1;32m    252\u001b[0m b \u001b[38;5;241m=\u001b[39m b \u001b[38;5;241m+\u001b[39m s\n\u001b[0;32m--> 253\u001b[0m n, err_code \u001b[38;5;241m=\u001b[39m \u001b[43mdecoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    255\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "trainDIRs = ['../../../AD_NC/train/AD/', '../../../AD_NC/train/NC']\n",
    "testDIRs = ['../../../AD_NC/test/AD/', '../../../AD_NC/test/NC']\n",
    "xtrain, ytrain, xtest, ytest = getImages(trainDIRs, testDIRs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8790aae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "xtrain = flattenPatches(createPatches(xtrain, (16,16)))\n",
    "xtest = flattenPatches(createPatches(xtest, (16,16)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5e33b2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainloader(batchsize=16):\n",
    "    return DataLoader(DatasetWrapper(xtrain, ytrain), batchsize=batchsize, shuffle=True)\n",
    "\n",
    "def testloader():\n",
    "    return DataLoader(DatasetWrapper(xtest, ytest), batchsize=1, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e722bf46",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainshape():\n",
    "    return xtrain.shape\n",
    "\n",
    "def testshape():\n",
    "    return xtest.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d704f722",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
