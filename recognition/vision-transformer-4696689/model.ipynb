{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "206e485b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"numpy and torch\"\"\"\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "\"\"\"PIL\"\"\"\n",
    "from PIL import Image\n",
    "\n",
    "\"\"\"torchvision and utils\"\"\"\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision.utils import make_grid\n",
    "from torchvision.utils import save_image\n",
    "\n",
    "\"\"\"os\"\"\"\n",
    "import os\n",
    "\n",
    "\n",
    "\"\"\"Get image to tensor\"\"\"\n",
    "transform = transforms.Compose([\n",
    "    transforms.PILToTensor()\n",
    "])\n",
    "\n",
    "\"\"\"Loading data into arrays\"\"\"\n",
    "xtrain, xtrain, xtest, ytest = [], [], [], []\n",
    "\n",
    "\"\"\"training data\"\"\"\n",
    "trainDIRs = ['../../../AD_NC/train/AD/', '../../../AD_NC/train/NC']\n",
    "size = [0, 0]\n",
    "for i, DIR in enumerate(trainDIRs):\n",
    "    for filename in os.listdir(DIR):\n",
    "        f = os.path.join(DIR, filename)\n",
    "        img = Image.open(f)\n",
    "        tensor = transform(img).float()\n",
    "        tensor.require_grad = True\n",
    "        xtrain.append(tensor/255)\n",
    "        size[i] += 1\n",
    "xtrain = torch.stack(xtrain)\n",
    "ytrain = torch.from_numpy(np.concatenate((np.ones(size[0]), np.zeros(size[1])), axis=0))\n",
    "\n",
    "\"\"\"testing data\"\"\"\n",
    "testDIRs = ['../../../AD_NC/test/AD/', '../../../AD_NC/test/NC']\n",
    "size = [0, 0]\n",
    "for i, DIR in enumerate(testDIRs):\n",
    "    for filename in os.listdir(DIR):\n",
    "        f = os.path.join(DIR, filename)\n",
    "        img = Image.open(f)\n",
    "        tensor = transform(img).float()\n",
    "        tensor.require_grad = True\n",
    "        xtest.append(tensor/255)\n",
    "        size[i] += 1\n",
    "xtest = torch.stack(xtest)\n",
    "ytest = torch.from_numpy(np.concatenate((np.ones(size[0]), np.zeros(size[1])), axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e4ae7366",
   "metadata": {},
   "outputs": [],
   "source": [
    "def createPatches(imgs, wsize, hsize):\n",
    "    N, C, W, H = imgs.shape #number imgs, channels, width, height\n",
    "    size = (N, C, W // wsize, wsize, H // hsize, hsize)\n",
    "    perm = (0, 2, 4, 1, 3, 5) #bring col, row index of patch to front\n",
    "    flat = (1, 2) #flatten (col, row) index into col*row entry index for patches\n",
    "    imgs = imgs.reshape(size).permute(perm).flatten(*flat)\n",
    "    return imgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cda6b4d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "patches = createPatches(xtrain, 16, 16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b75b1f37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "whole thing\n",
      "torch.Size([21520, 1, 240, 256])\n",
      "torch.Size([21520, 240, 1, 16, 16])\n",
      "individual image\n",
      "torch.Size([1, 240, 256])\n",
      "torch.Size([240, 1, 16, 16])\n"
     ]
    }
   ],
   "source": [
    "print(\"whole thing\")\n",
    "print(xtrain.shape)\n",
    "print(patches.shape)\n",
    "print(\"individual image\")\n",
    "print(xtrain[0].shape)\n",
    "print(patches[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4a5e269a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "def flattenPatches(imgs): #takes input (N, Npatches, C, W, H)\n",
    "    return imgs.flatten(2, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "77000fbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([21520, 240, 256])\n"
     ]
    }
   ],
   "source": [
    "flattenedpatches = flattenPatches(patches)\n",
    "print(flattenedpatches.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5459379c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"projecting the patches to tokens\"\"\"\n",
    "EMBED_DIMENSION = 123\n",
    "wsize, hsize = 16, 16\n",
    "N, C, W, H = xtrain.shape\n",
    "proj = nn.Linear(C*wsize*hsize, EMBED_DIMENSION)\n",
    "tokens = proj(flattenedpatches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a9140821",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([21520, 240, 123])\n"
     ]
    }
   ],
   "source": [
    "print(tokens.shape) #of the form N, Ntokens, EMBED_DIMENSION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "48408915",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"adding the class tokens\"\"\"\n",
    "clstoken = nn.Parameter(torch.zeros(1, 1, EMBED_DIMENSION))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "44377362",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"positional embedding\"\"\"\n",
    "def embedding(npatches, EMBED_DIMENSION, freq):\n",
    "    posembed = torch.zeros(npatches, EMBED_DIMENSION)\n",
    "    for i in range(npatches):\n",
    "        for j in range(EMBED_DIMENSION):\n",
    "            if j % 2 == 0:\n",
    "                posembed[i][j] = np.sin(i/(freq**(j/EMBED_DIMENSION)))\n",
    "            else:\n",
    "                posembed[i][j] = np.cos(i/(freq**((j-1)/EMBED_DIMENSION)))\n",
    "    return posembed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "67f42615",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 6])\n"
     ]
    }
   ],
   "source": [
    "#test the positional embedding\n",
    "embed = embedding(4, 6, 10)\n",
    "print(embed.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "be1ad77d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Vision Transformer Class to create a vision transformer model\n",
    "\"\"\"\n",
    "class VisionTransformer(nn.Module):\n",
    "    def __init__(self, imgsize, patchsize):\n",
    "        super().__init__()\n",
    "        (self.N, self.C, self.W, self.H) = imgsize\n",
    "        (self.wsize, self.hsize) = patchsize\n",
    "        \"\"\"check for errors with sizing\"\"\"\n",
    "        if (W % wsize != 0) or (H % hsize != 0):\n",
    "            raise Exception(\"patchsize is not appropriate\")\n",
    "        if (self.C != C) or (self.H != H):\n",
    "            raise Exception(\"given sizes do not match\")\n",
    "        \"\"\"components\"\"\"\n",
    "        self.proj = nn.Linear(self.C*self.wsize*self.hsize, EMBED_DIMENSION)\n",
    "        self.clstoken = nn.Parameter(torch.zeros(1, 1, EMBED_DIMENSION))\n",
    "        Np = (self.W // wsize) * (self.H // hsize)\n",
    "        self.posembed = embedding(Np+1, EMBED_DIMENSION, freq=10000) #10000 is described in ViT paper\n",
    "        self.posembed = self.posembed.repeat(N, 1, 1)\n",
    "    \n",
    "    def createPatches(self, imgs):\n",
    "        size = (self.N, self.C, self.W // self.wsize, self.wsize, self.H // self.hsize, self.hsize)\n",
    "        perm = (0, 2, 4, 1, 3, 5) #bring col, row index of patch to front\n",
    "        flat = (1, 2) #flatten (col, row) index into col*row entry index for patches\n",
    "        imgs = imgs.reshape(size).permute(perm).flatten(*flat)\n",
    "        return imgs #in format Nimgs, Npatches, C, Wpatch, Hpatch\n",
    "    \n",
    "    def flattenPatches(self, imgs): #takes input (N, Npatches, C, W, H)\n",
    "        return imgs.flatten(2, 4)\n",
    "    \n",
    "    def embedding(npatches, EMBED_DIMENSION, freq):\n",
    "        posembed = torch.zeros(npatches, EMBED_DIMENSION)\n",
    "        for i in range(npatches):\n",
    "            for j in range(EMBED_DIMENSION):\n",
    "                if j % 2 == 0:\n",
    "                    posembed[i][j] = np.sin(i/(freq**(j/EMBED_DIMENSION)))\n",
    "                else:\n",
    "                    posembed[i][j] = np.cos(i/(freq**((j-1)/EMBED_DIMENSION)))\n",
    "        return posembed\n",
    "    \n",
    "    def forward(self, imgs, prepatched=True): #assume size checking done by createPatches\n",
    "        if not prepatched:\n",
    "            imgs = self.createPatches(imgs) #create patches\n",
    "            imgs = self.flattenPatches(imgs) #flatten patch C,W,H into one array\n",
    "        \"\"\"Linear Projection and Positional Embedding\"\"\"\n",
    "        tokens = self.proj(imgs) #perform linear projection\n",
    "        N, Np, P = tokens.shape\n",
    "        clstoken = self.clstoken.repeat(N, 1, 1)\n",
    "        tokens = torch.cat([clstoken, tokens], dim=1) #concat the class token\n",
    "        tokens = tokens + self.posembed #add positional encoding\n",
    "        \"\"\"Transformer\"\"\"\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e88c7967",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokens shape: torch.Size([21520, 240, 123])\n",
      "cls shape: torch.Size([21520, 1, 123])\n",
      "tokens shape: torch.Size([21520, 241, 123])\n",
      "tokens+embed shape: torch.Size([21520, 241, 123])\n"
     ]
    }
   ],
   "source": [
    "patchsize = (16, 16)\n",
    "ViT = VisionTransformer(xtrain.shape, patchsize)\n",
    "ViT.forward(xtrain, prepatched=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "32a484b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, heads, EMBED_DIMENSION):\n",
    "        super().__init__()\n",
    "        self.heads = heads\n",
    "        self.attn = nn.MultiheadAttention(EMBED_DIMENSION, heads, batch_first=True)\n",
    "        self.Q = nn.Linear(EMBED_DIMENSION, EMBED_DIMENSION, bias=False)\n",
    "        self.K = nn.Linear(EMBED_DIMENSION, EMBED_DIMENSION, bias=False)\n",
    "        self.V = nn.Linear(EMBED_DIMENSION, EMBED_DIMENSION, bias=False)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        Q = self.Q(x)\n",
    "        K = self.K(x)\n",
    "        V = self.V(x)\n",
    "        \n",
    "        attnout, attnweights = self.attn(Q, K, V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49b11d09",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransBlock(nn.Module):\n",
    "    def __init__(self, heads, EMBED_DIMENSION, fflsize)\n",
    "        super().__init__()\n",
    "        self.fnorm = nn.LayerNorm(EMBED_DIMENSION)\n",
    "        self.snorm = nn.LayerNorm(EMBED_DIMENSION)\n",
    "        self.attn = Attention(heads, EMBED_DIMENSION)\n",
    "        self.ffl = nn.Sequential(\n",
    "            nn.Linear(EMBED_DIMENSION, fflsize),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(fflsize, EMBED_DIMENSION)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Switching to pre-MHA LayerNorm is supposed to give better performance,\n",
    "        this is used in other models such as LLMs like GPT. Gradients are meant\n",
    "        to be stabilised. This is different to the original ViT paper.\n",
    "        \"\"\"\n",
    "        x = x + self.attn(self.fnorm(x))\n",
    "        x = x + self.ffl(self.snorm(x))\n",
    "        return x"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
