{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "206e485b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"numpy and torch\"\"\"\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "\"\"\"PIL\"\"\"\n",
    "from PIL import Image\n",
    "\n",
    "\"\"\"torchvision and utils\"\"\"\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision.utils import make_grid\n",
    "from torchvision.utils import save_image\n",
    "\n",
    "\"\"\"os\"\"\"\n",
    "import os\n",
    "\n",
    "\n",
    "\"\"\"Get image to tensor\"\"\"\n",
    "transform = transforms.Compose([\n",
    "    transforms.PILToTensor()\n",
    "])\n",
    "\n",
    "\"\"\"Loading data into arrays\"\"\"\n",
    "xtrain, xtrain, xtest, ytest = [], [], [], []\n",
    "\n",
    "\"\"\"training data\"\"\"\n",
    "trainDIRs = ['../../../AD_NC/train/AD/', '../../../AD_NC/train/NC']\n",
    "size = [0, 0]\n",
    "for i, DIR in enumerate(trainDIRs):\n",
    "    for filename in os.listdir(DIR):\n",
    "        f = os.path.join(DIR, filename)\n",
    "        img = Image.open(f)\n",
    "        tensor = transform(img).float()\n",
    "        tensor.require_grad = True\n",
    "        xtrain.append(tensor/255)\n",
    "        size[i] += 1\n",
    "xtrain = torch.stack(xtrain)\n",
    "ytrain = torch.from_numpy(np.concatenate((np.ones(size[0]), np.zeros(size[1])), axis=0))\n",
    "\n",
    "\"\"\"testing data\"\"\"\n",
    "testDIRs = ['../../../AD_NC/test/AD/', '../../../AD_NC/test/NC']\n",
    "size = [0, 0]\n",
    "for i, DIR in enumerate(testDIRs):\n",
    "    for filename in os.listdir(DIR):\n",
    "        f = os.path.join(DIR, filename)\n",
    "        img = Image.open(f)\n",
    "        tensor = transform(img).float()\n",
    "        tensor.require_grad = True\n",
    "        xtest.append(tensor/255)\n",
    "        size[i] += 1\n",
    "xtest = torch.stack(xtest)\n",
    "ytest = torch.from_numpy(np.concatenate((np.ones(size[0]), np.zeros(size[1])), axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e4ae7366",
   "metadata": {},
   "outputs": [],
   "source": [
    "def createPatches(imgs, wsize, hsize):\n",
    "    N, C, W, H = imgs.shape #number imgs, channels, width, height\n",
    "    size = (N, C, W // wsize, wsize, H // hsize, hsize)\n",
    "    perm = (0, 2, 4, 1, 3, 5) #bring col, row index of patch to front\n",
    "    flat = (1, 2) #flatten (col, row) index into col*row entry index for patches\n",
    "    imgs = imgs.reshape(size).permute(perm).flatten(*flat)\n",
    "    return imgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7cf030d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "patches = createPatches(xtrain, 16, 16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "081275a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "whole thing\n",
      "torch.Size([21520, 1, 240, 256])\n",
      "torch.Size([21520, 240, 1, 16, 16])\n",
      "individual image\n",
      "torch.Size([1, 240, 256])\n",
      "torch.Size([240, 1, 16, 16])\n"
     ]
    }
   ],
   "source": [
    "print(\"whole thing\")\n",
    "print(xtrain.shape)\n",
    "print(patches.shape)\n",
    "print(\"individual image\")\n",
    "print(xtrain[0].shape)\n",
    "print(patches[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9c9ac8df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "def flattenPatches(imgs): #takes input (N, Npatches, C, W, H)\n",
    "    return imgs.flatten(2, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0f3c8de9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([21520, 240, 256])\n"
     ]
    }
   ],
   "source": [
    "flattenedpatches = flattenPatches(patches)\n",
    "print(flattenedpatches.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fe04de85",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"projecting the patches to tokens\"\"\"\n",
    "EMBED_DIMENSION = 123\n",
    "wsize, hsize = 16, 16\n",
    "N, C, W, H = xtrain.shape\n",
    "proj = nn.Linear(C*wsize*hsize, EMBED_DIMENSION)\n",
    "tokens = proj(flattenedpatches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "884f3bf7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([21520, 240, 123])\n"
     ]
    }
   ],
   "source": [
    "print(tokens.shape) #of the form N, Ntokens, EMBED_DIMENSION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "055cb6ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"adding the class tokens\"\"\"\n",
    "clstoken = nn.Parameter(torch.zeros(1, 1, EMBED_DIMENSION))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d3b08350",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"positional embedding\"\"\"\n",
    "def embedding(npatches, EMBED_DIMENSION, freq):\n",
    "    posembed = torch.zeros(npatches, EMBED_DIMENSION)\n",
    "    for i in range(npatches):\n",
    "        for j in range(EMBED_DIMENSION):\n",
    "            if j % 2 == 0:\n",
    "                posembed[i][j] = np.sin(i/(freq**(j/EMBED_DIMENSION)))\n",
    "            else:\n",
    "                posembed[i][j] = np.cos(i/(freq**((j-1)/EMBED_DIMENSION)))\n",
    "    return posembed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0d529c3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 6])\n"
     ]
    }
   ],
   "source": [
    "#test the positional embedding\n",
    "embed = embedding(4, 6, 10)\n",
    "print(embed.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "caf9b479",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Vision Transformer Class to create a vision transformer model\n",
    "\"\"\"\n",
    "class VisionTransformer(nn.Module):\n",
    "    def __init__(self, imgsize, patchsize):\n",
    "        super().__init__()\n",
    "        (self.N, self.C, self.W, self.H) = imgsize\n",
    "        (self.wsize, self.hsize) = patchsize\n",
    "        \"\"\"check for errors with sizing\"\"\"\n",
    "        if (W % wsize != 0) or (H % hsize != 0):\n",
    "            raise Exception(\"patchsize is not appropriate\")\n",
    "        if (self.C != C) or (self.H != H):\n",
    "            raise Exception(\"given sizes do not match\")\n",
    "        \"\"\"components\"\"\"\n",
    "        self.proj = nn.Linear(self.C*self.wsize*self.hsize, EMBED_DIMENSION)\n",
    "        self.clstoken = nn.Parameter(torch.zeros(1, 1, EMBED_DIMENSION))\n",
    "        Np = (self.W // wsize) * (self.H // hsize)\n",
    "        self.posembed = embedding(Np+1, EMBED_DIMENSION, freq=10000) #10000 is described in ViT paper\n",
    "        self.posembed = self.posembed.repeat(N, 1, 1)\n",
    "    \n",
    "    def createPatches(self, imgs):\n",
    "        size = (self.N, self.C, self.W // self.wsize, self.wsize, self.H // self.hsize, self.hsize)\n",
    "        perm = (0, 2, 4, 1, 3, 5) #bring col, row index of patch to front\n",
    "        flat = (1, 2) #flatten (col, row) index into col*row entry index for patches\n",
    "        imgs = imgs.reshape(size).permute(perm).flatten(*flat)\n",
    "        return imgs #in format Nimgs, Npatches, C, Wpatch, Hpatch\n",
    "    \n",
    "    def flattenPatches(self, imgs): #takes input (N, Npatches, C, W, H)\n",
    "        return imgs.flatten(2, 4)\n",
    "    \n",
    "    def embedding(npatches, EMBED_DIMENSION, freq):\n",
    "        posembed = torch.zeros(npatches, EMBED_DIMENSION)\n",
    "        for i in range(npatches):\n",
    "            for j in range(EMBED_DIMENSION):\n",
    "                if j % 2 == 0:\n",
    "                    posembed[i][j] = np.sin(i/(freq**(j/EMBED_DIMENSION)))\n",
    "                else:\n",
    "                    posembed[i][j] = np.cos(i/(freq**((j-1)/EMBED_DIMENSION)))\n",
    "        return posembed\n",
    "    \n",
    "    def forward(self, imgs, prepatched=True): #assume size checking done by createPatches\n",
    "        if not prepatched:\n",
    "            imgs = self.createPatches(imgs)\n",
    "            imgs = self.flattenPatches(imgs)\n",
    "        \"\"\"projection embedding\"\"\"\n",
    "        tokens = self.proj(imgs)\n",
    "        print(\"tokens shape:\", tokens.shape)\n",
    "        N, Np, P = tokens.shape\n",
    "        clstoken = self.clstoken.repeat(N, 1, 1)\n",
    "        print(\"cls shape:\", clstoken.shape)\n",
    "        tokens = torch.cat([clstoken, tokens], dim=1)\n",
    "        print(\"tokens shape:\", tokens.shape)\n",
    "        tokens = tokens + self.posembed\n",
    "        print(\"tokens+embed shape:\", tokens.shape)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "caab0810",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokens shape: torch.Size([21520, 240, 123])\n",
      "cls shape: torch.Size([21520, 1, 123])\n",
      "tokens shape: torch.Size([21520, 241, 123])\n",
      "tokens+embed shape: torch.Size([21520, 241, 123])\n"
     ]
    }
   ],
   "source": [
    "patchsize = (16, 16)\n",
    "ViT = VisionTransformer(xtrain.shape, patchsize)\n",
    "ViT.forward(xtrain, prepatched=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b226909e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
