{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "206e485b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"numpy and torch\"\"\"\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "\"\"\"PIL\"\"\"\n",
    "from PIL import Image\n",
    "\n",
    "\"\"\"torchvision and utils\"\"\"\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision.utils import make_grid\n",
    "from torchvision.utils import save_image\n",
    "\n",
    "\"\"\"os\"\"\"\n",
    "import os\n",
    "\n",
    "\n",
    "\"\"\"Get image to tensor\"\"\"\n",
    "transform = transforms.Compose([\n",
    "    transforms.PILToTensor()\n",
    "])\n",
    "\n",
    "\"\"\"Loading data into arrays\"\"\"\n",
    "xtrain, xtrain, xtest, ytest = [], [], [], []\n",
    "\n",
    "\"\"\"training data\"\"\"\n",
    "trainDIRs = ['../../../AD_NC/train/AD/', '../../../AD_NC/train/NC']\n",
    "size = [0, 0]\n",
    "for i, DIR in enumerate(trainDIRs):\n",
    "    for filename in os.listdir(DIR):\n",
    "        f = os.path.join(DIR, filename)\n",
    "        img = Image.open(f)\n",
    "        tensor = transform(img).float()\n",
    "        tensor.require_grad = True\n",
    "        xtrain.append(tensor/255)\n",
    "        size[i] += 1\n",
    "xtrain = torch.stack(xtrain)\n",
    "ytrain = torch.from_numpy(np.concatenate((np.ones(size[0]), np.zeros(size[1])), axis=0))\n",
    "\n",
    "\"\"\"testing data\"\"\"\n",
    "testDIRs = ['../../../AD_NC/test/AD/', '../../../AD_NC/test/NC']\n",
    "size = [0, 0]\n",
    "for i, DIR in enumerate(testDIRs):\n",
    "    for filename in os.listdir(DIR):\n",
    "        f = os.path.join(DIR, filename)\n",
    "        img = Image.open(f)\n",
    "        tensor = transform(img).float()\n",
    "        tensor.require_grad = True\n",
    "        xtest.append(tensor/255)\n",
    "        size[i] += 1\n",
    "xtest = torch.stack(xtest)\n",
    "ytest = torch.from_numpy(np.concatenate((np.ones(size[0]), np.zeros(size[1])), axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "e4ae7366",
   "metadata": {},
   "outputs": [],
   "source": [
    "def createPatches(imgs, wsize, hsize):\n",
    "    N, C, W, H = imgs.shape #number imgs, channels, width, height\n",
    "    size = (N, C, W // wsize, wsize, H // hsize, hsize)\n",
    "    perm = (0, 2, 4, 1, 3, 5) #bring col, row index of patch to front\n",
    "    flat = (1, 2) #flatten (col, row) index into col*row entry index for patches\n",
    "    imgs = imgs.reshape(size).permute(perm).flatten(*flat)\n",
    "    return imgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "ed1fa920",
   "metadata": {},
   "outputs": [],
   "source": [
    "patches = createPatches(xtrain, 16, 16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "1734a153",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "whole thing\n",
      "torch.Size([21520, 1, 240, 256])\n",
      "torch.Size([21520, 240, 1, 16, 16])\n",
      "individual image\n",
      "torch.Size([1, 240, 256])\n",
      "torch.Size([240, 1, 16, 16])\n"
     ]
    }
   ],
   "source": [
    "print(\"whole thing\")\n",
    "print(xtrain.shape)\n",
    "print(patches.shape)\n",
    "print(\"individual image\")\n",
    "print(xtrain[0].shape)\n",
    "print(patches[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "2e6836c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "def flattenPatches(imgs): #takes input (N, Npatches, C, W, H)\n",
    "    return imgs.flatten(2, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "3b55bc20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([21520, 240, 256])\n"
     ]
    }
   ],
   "source": [
    "flattenedpatches = flattenPatches(patches)\n",
    "print(flattenedpatches.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "87f60f02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([21520, 240, 123])\n"
     ]
    }
   ],
   "source": [
    "\"\"\"projecting the patches to tokens\"\"\"\n",
    "EMBED_DIMENSION = 123\n",
    "wsize, hsize = 16, 16\n",
    "N, C, W, H = xtrain.shape\n",
    "proj = nn.Linear(C*wsize*hsize, EMBED_DIMENSION)\n",
    "tokens = proj(flattenedpatches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "b6d9e2f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([21520, 240, 123])\n"
     ]
    }
   ],
   "source": [
    "print(tokens.shape) #of the form N, Ntokens, EMBED_DIMENSION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78efdb7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"adding the class tokens\"\"\"\n",
    "clstoken = nn.Parameter(torch.zeros(1, 1, EMBED_DIMENSION))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "34c6a456",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"positional embedding\"\"\"\n",
    "def embedding(npatches, EMBED_DIMENSION, freq):\n",
    "    posembed = torch.zeros(npatches, EMBED_DIMENSION)\n",
    "    for i in range(npatches):\n",
    "        for j in range(EMBED_DIMENSION):\n",
    "            if j % 2 == 0:\n",
    "                posembed[i][j] = np.sin(i/(freq**(j/EMBED_DIMENSION)))\n",
    "            else:\n",
    "                posembed[i][j] = np.cos(i/(freq**((j-1)/EMBED_DIMENSION)))\n",
    "    return posembed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "b84bded5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.0000,  1.0000,  0.0000,  1.0000,  0.0000,  1.0000],\n",
      "        [ 0.8415,  0.5403,  0.4477,  0.8942,  0.2138,  0.9769],\n",
      "        [ 0.9093, -0.4161,  0.8006,  0.5992,  0.4177,  0.9086],\n",
      "        [ 0.1411, -0.9900,  0.9841,  0.1774,  0.6023,  0.7983]])\n"
     ]
    }
   ],
   "source": [
    "#test the positional embedding\n",
    "embed = embedding(4, 6, 10)\n",
    "print(embed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "078e74aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Vision Transformer Class to create a vision transformer model\n",
    "\"\"\"\n",
    "class VisionTransformer(nn.Module):\n",
    "    def __init__(self, imgsize, patchsize):\n",
    "        super().__init__()\n",
    "        self.C, self.W, self.H = *imgsize\n",
    "        self.wsize, self.hsize = *patchsize\n",
    "        \"\"\"components\"\"\"\n",
    "        self.proj = nn.Linear(self.C*self.W*self.H, EMBED_DIMENSION)\n",
    "        self.clstoken = nn.Parameter(torch.zeros(1, 1, EMBED_DIMENSION))\n",
    "    \n",
    "    def createPatches(self, imgs, wsize, hsize):\n",
    "        N, C, W, H = imgs.shape\n",
    "        if (W % wsize != 0) or (H % hsize != 0):\n",
    "            raise Exception(\"patchsize is not appropriate\")\n",
    "        else if (self.C != C) or (self.H != H):\n",
    "            raise Exception(\"given sizes do not match\")\n",
    "        \"\"\"if everything ok\"\"\"\n",
    "        size = (N, C, W // wsize, wsize, H // hsize, hsize)\n",
    "        perm = (0, 2, 4, 1, 3, 5) #bring col, row index of patch to front\n",
    "        flat = (1, 2) #flatten (col, row) index into col*row entry index for patches\n",
    "        imgs = imgs.reshape(size).permute(perm).flatten(*flat)\n",
    "        return imgs #in format Nimgs, Npatches, C, Wpatch, Hpatch\n",
    "    \n",
    "    def forward(self, imgs): #assume size checking done by createPatches\n",
    "        patches = self.createPatches(imgs, self.wsize, self.hsize)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
