{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x203297588f0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "from tqdm import tqdm, trange\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import Adam\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from torchvision.transforms import ToTensor\n",
    "from torchvision.datasets.mnist import MNIST\n",
    "\n",
    "np.random.seed(0)\n",
    "torch.manual_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torchvision import transforms, datasets\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "class ADNIDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "\n",
    "        # Get a list of subdirectories (labels) in the root directory\n",
    "        self.labels = sorted(os.listdir(root_dir))\n",
    "\n",
    "        # Create a mapping from label (subdirectory) to an integer\n",
    "        self.label_to_idx = {label: idx for idx, label in enumerate(self.labels)}\n",
    "\n",
    "        self.data = []\n",
    "        self.targets = []\n",
    "        self.names = []\n",
    "\n",
    "        # Create a dictionary to store the count of images for each class\n",
    "        self.class_counts = {label: 0 for label in self.labels}\n",
    "\n",
    "        # Iterate through subdirectories\n",
    "        for label in self.labels:\n",
    "            label_dir = os.path.join(root_dir, label)\n",
    "            \n",
    "            # Check if it's a directory\n",
    "            if not os.path.isdir(label_dir):\n",
    "                continue\n",
    "\n",
    "            # Get a list of image files in the subdirectory\n",
    "            image_files = [f for f in os.listdir(label_dir) if f.lower().endswith((\".jpg\", \".jpeg\", \".png\", \".gif\"))]\n",
    "\n",
    "            # Update class counts and append image paths and their corresponding labels\n",
    "            for image_file in image_files:\n",
    "                image_path = os.path.join(label_dir, image_file)\n",
    "                self.data.append(image_path)\n",
    "                self.targets.append(self.label_to_idx[label])\n",
    "                self.names.append(image_file)\n",
    "                self.class_counts[label] += 1\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_path = self.data[idx]\n",
    "        image = Image.open(image_path)\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        label = self.targets[idx]\n",
    "        name = self.names[idx]\n",
    "\n",
    "        return image, label, name\n",
    "\n",
    "    def get_class_counts(self):\n",
    "        return self.class_counts\n",
    "    \n",
    "    def get_labels(self):\n",
    "        return self.labels\n",
    "    \n",
    "    def get_names(self):\n",
    "        return self.names\n",
    "\n",
    "def create_dataloaders(dir, batch_size=64, shuffle=True, transform=None, num_workers=4):\n",
    "    train_path = os.path.join(dir, \"train\")\n",
    "    test_path = os.path.join(dir, \"test\")\n",
    "\n",
    "    train_dataset = ADNIDataset(train_path, transform=transform)\n",
    "    test_dataset = ADNIDataset(test_path, transform=transform)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=64, shuffle=False)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False, drop_last=True)\n",
    "\n",
    "    return train_loader, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading data\n",
    "transform = ToTensor()\n",
    "\n",
    "train_set = MNIST(root='D:/MNIST/', train=True, download=True, transform=transform)\n",
    "test_set = MNIST(root='D:/MNIST/', train=False, download=True, transform=transform)\n",
    "\n",
    "train_loader = DataLoader(train_set, shuffle=True, batch_size=BATCH_SIZE)\n",
    "test_loader = DataLoader(test_set, shuffle=False, batch_size=BATCH_SIZE)\n",
    "\n",
    "# transform = transforms.Compose([\n",
    "#     transforms.CenterCrop((192, 192)),\n",
    "#     # transforms.RandomRotation(degrees=90),\n",
    "#     transforms.ToTensor(),\n",
    "# ])\n",
    "\n",
    "# train_loader, test_loader = create_dataloaders(\"D:/AD_NC\", transform=transform)\n",
    "# print(len(test_loader))\n",
    "# print(train_loader.dataset.get_class_counts())\n",
    "# print(test_loader.dataset.get_class_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def patchify(images, n_patches):\n",
    "    n, c, h, w = images.shape\n",
    "\n",
    "    assert h == w, \"Patchify method is implemented for square images only\"\n",
    "\n",
    "    patches = torch.zeros(n, n_patches ** 2, h * w * c // n_patches ** 2)\n",
    "    patch_size = h // n_patches\n",
    "\n",
    "    for idx, image in enumerate(images):\n",
    "        for i in range(n_patches):\n",
    "            for j in range(n_patches):\n",
    "                patch = image[:, i * patch_size: (i + 1) * patch_size, j * patch_size: (j + 1) * patch_size]\n",
    "                patches[idx, i * n_patches + j] = patch.flatten()\n",
    "    return patches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_positional_embeddings(sequence_length, d):\n",
    "    result = torch.ones(sequence_length, d)\n",
    "    for i in range(sequence_length):\n",
    "        for j in range(d):\n",
    "            result[i][j] = np.sin(i / (10000 ** (j / d))) if j % 2 == 0 else np.cos(i / (10000 ** ((j - 1) / d)))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyMSA(nn.Module):\n",
    "    def __init__(self, d, n_heads=2):\n",
    "        super(MyMSA, self).__init__()\n",
    "        self.d = d\n",
    "        self.n_heads = n_heads\n",
    "\n",
    "        assert d % n_heads == 0, f\"Can't divide dimension {d} into {n_heads} heads\"\n",
    "\n",
    "        d_head = int(d / n_heads)\n",
    "        self.q_mappings = nn.ModuleList([nn.Linear(d_head, d_head) for _ in range(self.n_heads)])\n",
    "        self.k_mappings = nn.ModuleList([nn.Linear(d_head, d_head) for _ in range(self.n_heads)])\n",
    "        self.v_mappings = nn.ModuleList([nn.Linear(d_head, d_head) for _ in range(self.n_heads)])\n",
    "        self.d_head = d_head\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "    def forward(self, sequences):\n",
    "        # Sequences has shape (N, seq_length, token_dim)\n",
    "        # We go into shape    (N, seq_length, n_heads, token_dim / n_heads)\n",
    "        # And come back to    (N, seq_length, item_dim)  (through concatenation)\n",
    "        result = []\n",
    "        for sequence in sequences:\n",
    "            seq_result = []\n",
    "            for head in range(self.n_heads):\n",
    "                q_mapping = self.q_mappings[head]\n",
    "                k_mapping = self.k_mappings[head]\n",
    "                v_mapping = self.v_mappings[head]\n",
    "\n",
    "                seq = sequence[:, head * self.d_head: (head + 1) * self.d_head]\n",
    "                q, k, v = q_mapping(seq), k_mapping(seq), v_mapping(seq)\n",
    "\n",
    "                attention = self.softmax(q @ k.T / (self.d_head ** 0.5))\n",
    "                seq_result.append(attention @ v)\n",
    "            result.append(torch.hstack(seq_result))\n",
    "        return torch.cat([torch.unsqueeze(r, dim=0) for r in result])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyViTBlock(nn.Module):\n",
    "    def __init__(self, hidden_d, n_heads, mlp_ratio=4):\n",
    "        super(MyViTBlock, self).__init__()\n",
    "        self.hidden_d = hidden_d\n",
    "        self.n_heads = n_heads\n",
    "\n",
    "        self.norm1 = nn.LayerNorm(hidden_d)\n",
    "        self.mhsa = MyMSA(hidden_d, n_heads)\n",
    "        self.norm2 = nn.LayerNorm(hidden_d)\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(hidden_d, mlp_ratio * hidden_d),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(mlp_ratio * hidden_d, hidden_d)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = x + self.mhsa(self.norm1(x))\n",
    "        out = out + self.mlp(self.norm2(out))\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyViT(nn.Module):\n",
    "    def __init__(self, chw, n_patches=7, n_blocks=2, hidden_d=8, n_heads=2, out_d=10):\n",
    "        # Super constructor\n",
    "        super(MyViT, self).__init__()\n",
    "        \n",
    "        # Attributes\n",
    "        self.chw = chw # ( C , H , W )\n",
    "        self.n_patches = n_patches\n",
    "        self.n_blocks = n_blocks\n",
    "        self.n_heads = n_heads\n",
    "        self.hidden_d = hidden_d\n",
    "        \n",
    "        # Input and patches sizes\n",
    "        assert chw[1] % n_patches == 0, \"Input shape not entirely divisible by number of patches\"\n",
    "        assert chw[2] % n_patches == 0, \"Input shape not entirely divisible by number of patches\"\n",
    "        self.patch_size = (chw[1] / n_patches, chw[2] / n_patches)\n",
    "\n",
    "        # 1) Linear mapper\n",
    "        self.input_d = int(chw[0] * self.patch_size[0] * self.patch_size[1])\n",
    "        self.linear_mapper = nn.Linear(self.input_d, self.hidden_d)\n",
    "        \n",
    "        # 2) Learnable classification token\n",
    "        self.class_token = nn.Parameter(torch.rand(1, self.hidden_d))\n",
    "        \n",
    "        # 3) Positional embedding\n",
    "        self.register_buffer('positional_embeddings', get_positional_embeddings(n_patches ** 2 + 1, hidden_d), persistent=False)\n",
    "        \n",
    "        # 4) Transformer encoder blocks\n",
    "        self.blocks = nn.ModuleList([MyViTBlock(hidden_d, n_heads) for _ in range(n_blocks)])\n",
    "        \n",
    "        # 5) Classification MLPk\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(self.hidden_d, out_d),\n",
    "            nn.Softmax(dim=-1)\n",
    "        )\n",
    "\n",
    "    def forward(self, images):\n",
    "        # Dividing images into patches\n",
    "        n, c, h, w = images.shape\n",
    "        patches = patchify(images, self.n_patches).to(self.positional_embeddings.device)\n",
    "        \n",
    "        # Running linear layer tokenization\n",
    "        # Map the vector corresponding to each patch to the hidden size dimension\n",
    "        tokens = self.linear_mapper(patches)\n",
    "        \n",
    "        # Adding classification token to the tokens\n",
    "        tokens = torch.cat((self.class_token.expand(n, 1, -1), tokens), dim=1)\n",
    "        \n",
    "        # Adding positional embedding\n",
    "        out = tokens + self.positional_embeddings.repeat(n, 1, 1)\n",
    "        \n",
    "        # Transformer Blocks\n",
    "        for block in self.blocks:\n",
    "            out = block(out)\n",
    "            \n",
    "        # Getting the classification token only\n",
    "        out = out[:, 0]\n",
    "        \n",
    "        return self.mlp(out) # Map to output dimension, output category distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device:  cuda (NVIDIA GeForce RTX 3060 Laptop GPU)\n"
     ]
    }
   ],
   "source": [
    "# Defining model and training options\n",
    "NUM_LAYERS = 2\n",
    "NUM_PATCHES = 7\n",
    "IMAGE_CHANNELS = 1\n",
    "IMAGE_SIZE = 28\n",
    "PATCH_SIZE = 12\n",
    "EMBEDDING_DIMS = IMAGE_CHANNELS * PATCH_SIZE**2 # Hidden Size D\n",
    "MAN_HIDDEN_DIMS = 8\n",
    "# print(EMBEDDING_DIMS)\n",
    "NUM_HEADS = 2\n",
    "NUM_CLASSES = 10\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device: \", device, f\"({torch.cuda.get_device_name(device)})\" if torch.cuda.is_available() else \"\")\n",
    "model = MyViT((IMAGE_CHANNELS, IMAGE_SIZE, IMAGE_SIZE),\n",
    "            n_patches=NUM_PATCHES,\n",
    "            n_blocks=NUM_LAYERS, \n",
    "            hidden_d=MAN_HIDDEN_DIMS, \n",
    "            n_heads=NUM_HEADS, \n",
    "            out_d=NUM_CLASSES\n",
    "            ).to(device)\n",
    "N_EPOCHS = 5\n",
    "LR = 0.005\n",
    "optimizer = Adam(model.parameters(), lr=LR, weight_decay=1e-5)\n",
    "criterion = CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "=============================================================================================================================\n",
       "Layer (type (var_name))                       Input Shape          Output Shape         Param #              Trainable\n",
       "=============================================================================================================================\n",
       "MyViT (MyViT)                                 [128, 1, 28, 28]     [128, 10]            8                    True\n",
       "├─Linear (linear_mapper)                      [128, 49, 16]        [128, 49, 8]         136                  True\n",
       "├─ModuleList (blocks)                         --                   --                   --                   True\n",
       "│    └─MyViTBlock (0)                         [128, 50, 8]         [128, 50, 8]         --                   True\n",
       "│    │    └─LayerNorm (norm1)                 [128, 50, 8]         [128, 50, 8]         16                   True\n",
       "│    │    └─MyMSA (mhsa)                      [128, 50, 8]         [128, 50, 8]         120                  True\n",
       "│    │    └─LayerNorm (norm2)                 [128, 50, 8]         [128, 50, 8]         16                   True\n",
       "│    │    └─Sequential (mlp)                  [128, 50, 8]         [128, 50, 8]         552                  True\n",
       "│    └─MyViTBlock (1)                         [128, 50, 8]         [128, 50, 8]         --                   True\n",
       "│    │    └─LayerNorm (norm1)                 [128, 50, 8]         [128, 50, 8]         16                   True\n",
       "│    │    └─MyMSA (mhsa)                      [128, 50, 8]         [128, 50, 8]         120                  True\n",
       "│    │    └─LayerNorm (norm2)                 [128, 50, 8]         [128, 50, 8]         16                   True\n",
       "│    │    └─Sequential (mlp)                  [128, 50, 8]         [128, 50, 8]         552                  True\n",
       "├─Sequential (mlp)                            [128, 8]             [128, 10]            --                   True\n",
       "│    └─Linear (0)                             [128, 8]             [128, 10]            90                   True\n",
       "│    └─Softmax (1)                            [128, 10]            [128, 10]            --                   --\n",
       "=============================================================================================================================\n",
       "Total params: 1,642\n",
       "Trainable params: 1,642\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (Units.MEGABYTES): 1.71\n",
       "=============================================================================================================================\n",
       "Input size (MB): 0.40\n",
       "Forward/backward pass size (MB): 8.60\n",
       "Params size (MB): 0.01\n",
       "Estimated Total Size (MB): 9.01\n",
       "============================================================================================================================="
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchinfo import summary\n",
    "summary(model=model,\n",
    "        input_size=(BATCH_SIZE, IMAGE_CHANNELS, IMAGE_SIZE, IMAGE_SIZE), # (batch_size, num_patches, embedding_dimension)\n",
    "        col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n",
    "        col_width=20,\n",
    "        row_settings=[\"var_names\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 0/5 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "# Construct scaler for mixed precision training\n",
    "scaler = torch.cuda.amp.GradScaler()\n",
    "\n",
    "# Training loop\n",
    "for epoch in trange(N_EPOCHS, desc=\"Training\"):\n",
    "    train_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    batch_num = 0\n",
    "    \n",
    "    model.train()  # Set the model to training mode\n",
    "\n",
    "    for batch in tqdm(train_loader, desc=f\"Epoch {epoch + 1} in training\", leave=False):\n",
    "        batch_num += 1\n",
    "        x, y = batch\n",
    "        x, y = x.to(device), y.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        with torch.cuda.amp.autocast():\n",
    "            y_hat = model(x)\n",
    "            loss = criterion(y_hat, y)\n",
    "\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        _, predicted = y_hat.max(1)\n",
    "        total += y.size(0)\n",
    "        correct += predicted.eq(y).sum().item()\n",
    "\n",
    "        # if (batch_num % 100 == 0):\n",
    "        #     print(f\"Finished {batch_num} batches out of {len(train_loader)} batches\")\n",
    "\n",
    "    accuracy = 100 * correct / total\n",
    "    print(f\"Epoch {epoch + 1}/{N_EPOCHS} - Loss: {train_loss / len(train_loader):.4f}, Accuracy: {accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test loop\n",
    "with torch.no_grad():\n",
    "    correct, total = 0, 0\n",
    "    test_loss = 0.0\n",
    "    for batch in tqdm(test_loader, desc=\"Testing\"):\n",
    "        x, y = batch\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        y_hat = model(x)\n",
    "        loss = criterion(y_hat, y)\n",
    "        test_loss += loss.detach().cpu().item() / len(test_loader)\n",
    "\n",
    "        correct += torch.sum(torch.argmax(y_hat, dim=1) == y).detach().cpu().item()\n",
    "        total += len(x)\n",
    "    print(f\"Test loss: {test_loss:.2f}\")\n",
    "    print(f\"Test accuracy: {correct / total * 100:.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda-torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
