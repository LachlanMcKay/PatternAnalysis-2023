{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEVICE: cuda\n"
     ]
    }
   ],
   "source": [
    "# ==== import from package ==== #\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from einops import rearrange, repeat, reduce\n",
    "from collections import defaultdict\n",
    "# ==== import from this folder ==== #\n",
    "from model_VQVAE import VQVAE\n",
    "from discriminator import NLayerDiscriminator, weights_init\n",
    "from dataset import get_dataloader\n",
    "from util import reset_dir, weight_scheduler, compact_large_image\n",
    "from logger import Logger\n",
    "DEVICE = torch.device(\"cuda\")\n",
    "print(\"DEVICE:\", DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "called train_and_validate 32 None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9664/9664 [00:20<00:00, 466.55it/s]\n",
      "100%|██████████| 1120/1120 [00:01<00:00, 780.54it/s]\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "dataloader = get_dataloader(mode='train_and_validate', batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VQVAE(\n",
       "  (encoder): Encoder(\n",
       "    (conv_in): Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (down): Sequential(\n",
       "      (0): Sequential(\n",
       "        (0): ResnetBlock(\n",
       "          (block1): Sequential(\n",
       "            (0): GroupNorm(32, 64, eps=1e-05, affine=True)\n",
       "            (1): SiLU()\n",
       "            (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          )\n",
       "          (block2): Sequential(\n",
       "            (0): GroupNorm(32, 64, eps=1e-05, affine=True)\n",
       "            (1): SiLU()\n",
       "            (2): Dropout2d(p=0.0, inplace=False)\n",
       "            (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          )\n",
       "        )\n",
       "        (1): ResnetBlock(\n",
       "          (block1): Sequential(\n",
       "            (0): GroupNorm(32, 64, eps=1e-05, affine=True)\n",
       "            (1): SiLU()\n",
       "            (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          )\n",
       "          (block2): Sequential(\n",
       "            (0): GroupNorm(32, 64, eps=1e-05, affine=True)\n",
       "            (1): SiLU()\n",
       "            (2): Dropout2d(p=0.0, inplace=False)\n",
       "            (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          )\n",
       "        )\n",
       "        (2): Downsample(\n",
       "          (layer): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "        )\n",
       "      )\n",
       "      (1): Sequential(\n",
       "        (0): ResnetBlock(\n",
       "          (block1): Sequential(\n",
       "            (0): GroupNorm(32, 64, eps=1e-05, affine=True)\n",
       "            (1): SiLU()\n",
       "            (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          )\n",
       "          (block2): Sequential(\n",
       "            (0): GroupNorm(32, 64, eps=1e-05, affine=True)\n",
       "            (1): SiLU()\n",
       "            (2): Dropout2d(p=0.0, inplace=False)\n",
       "            (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          )\n",
       "        )\n",
       "        (1): ResnetBlock(\n",
       "          (block1): Sequential(\n",
       "            (0): GroupNorm(32, 64, eps=1e-05, affine=True)\n",
       "            (1): SiLU()\n",
       "            (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          )\n",
       "          (block2): Sequential(\n",
       "            (0): GroupNorm(32, 64, eps=1e-05, affine=True)\n",
       "            (1): SiLU()\n",
       "            (2): Dropout2d(p=0.0, inplace=False)\n",
       "            (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          )\n",
       "        )\n",
       "        (2): Downsample(\n",
       "          (layer): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "        )\n",
       "      )\n",
       "      (2): Sequential(\n",
       "        (0): ResnetBlock(\n",
       "          (block1): Sequential(\n",
       "            (0): GroupNorm(32, 64, eps=1e-05, affine=True)\n",
       "            (1): SiLU()\n",
       "            (2): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          )\n",
       "          (block2): Sequential(\n",
       "            (0): GroupNorm(32, 128, eps=1e-05, affine=True)\n",
       "            (1): SiLU()\n",
       "            (2): Dropout2d(p=0.0, inplace=False)\n",
       "            (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          )\n",
       "          (shortcut): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "        (1): ResnetBlock(\n",
       "          (block1): Sequential(\n",
       "            (0): GroupNorm(32, 128, eps=1e-05, affine=True)\n",
       "            (1): SiLU()\n",
       "            (2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          )\n",
       "          (block2): Sequential(\n",
       "            (0): GroupNorm(32, 128, eps=1e-05, affine=True)\n",
       "            (1): SiLU()\n",
       "            (2): Dropout2d(p=0.0, inplace=False)\n",
       "            (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          )\n",
       "        )\n",
       "        (2): Downsample(\n",
       "          (layer): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "        )\n",
       "      )\n",
       "      (3): Sequential(\n",
       "        (0): ResnetBlock(\n",
       "          (block1): Sequential(\n",
       "            (0): GroupNorm(32, 128, eps=1e-05, affine=True)\n",
       "            (1): SiLU()\n",
       "            (2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          )\n",
       "          (block2): Sequential(\n",
       "            (0): GroupNorm(32, 128, eps=1e-05, affine=True)\n",
       "            (1): SiLU()\n",
       "            (2): Dropout2d(p=0.0, inplace=False)\n",
       "            (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          )\n",
       "        )\n",
       "        (1): ResnetBlock(\n",
       "          (block1): Sequential(\n",
       "            (0): GroupNorm(32, 128, eps=1e-05, affine=True)\n",
       "            (1): SiLU()\n",
       "            (2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          )\n",
       "          (block2): Sequential(\n",
       "            (0): GroupNorm(32, 128, eps=1e-05, affine=True)\n",
       "            (1): SiLU()\n",
       "            (2): Dropout2d(p=0.0, inplace=False)\n",
       "            (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          )\n",
       "        )\n",
       "        (2): Downsample(\n",
       "          (layer): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "        )\n",
       "      )\n",
       "      (4): Sequential(\n",
       "        (0): ResnetBlock(\n",
       "          (block1): Sequential(\n",
       "            (0): GroupNorm(32, 128, eps=1e-05, affine=True)\n",
       "            (1): SiLU()\n",
       "            (2): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          )\n",
       "          (block2): Sequential(\n",
       "            (0): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
       "            (1): SiLU()\n",
       "            (2): Dropout2d(p=0.0, inplace=False)\n",
       "            (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          )\n",
       "          (shortcut): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "        (1): AttnBlock(\n",
       "          (norm): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
       "          (q): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (k): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (v): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (proj_out): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "        (2): ResnetBlock(\n",
       "          (block1): Sequential(\n",
       "            (0): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
       "            (1): SiLU()\n",
       "            (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          )\n",
       "          (block2): Sequential(\n",
       "            (0): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
       "            (1): SiLU()\n",
       "            (2): Dropout2d(p=0.0, inplace=False)\n",
       "            (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          )\n",
       "        )\n",
       "        (3): AttnBlock(\n",
       "          (norm): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
       "          (q): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (k): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (v): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (proj_out): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (mid): Sequential(\n",
       "      (0): ResnetBlock(\n",
       "        (block1): Sequential(\n",
       "          (0): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
       "          (1): SiLU()\n",
       "          (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (block2): Sequential(\n",
       "          (0): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
       "          (1): SiLU()\n",
       "          (2): Dropout2d(p=0.0, inplace=False)\n",
       "          (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "      )\n",
       "      (1): AttnBlock(\n",
       "        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
       "        (q): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (k): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (v): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (proj_out): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "      )\n",
       "      (2): ResnetBlock(\n",
       "        (block1): Sequential(\n",
       "          (0): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
       "          (1): SiLU()\n",
       "          (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (block2): Sequential(\n",
       "          (0): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
       "          (1): SiLU()\n",
       "          (2): Dropout2d(p=0.0, inplace=False)\n",
       "          (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (end): Sequential(\n",
       "      (0): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
       "      (1): SiLU()\n",
       "      (2): Conv2d(256, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    )\n",
       "  )\n",
       "  (conv_before_quant): Conv2d(16, 8, kernel_size=(1, 1), stride=(1, 1))\n",
       "  (conv_after_quant): Conv2d(8, 16, kernel_size=(1, 1), stride=(1, 1))\n",
       "  (decoder): Decoder(\n",
       "    (conv_in): Conv2d(16, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (mid): Sequential(\n",
       "      (0): ResnetBlock(\n",
       "        (block1): Sequential(\n",
       "          (0): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
       "          (1): SiLU()\n",
       "          (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (block2): Sequential(\n",
       "          (0): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
       "          (1): SiLU()\n",
       "          (2): Dropout2d(p=0.0, inplace=False)\n",
       "          (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "      )\n",
       "      (1): AttnBlock(\n",
       "        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
       "        (q): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (k): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (v): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (proj_out): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "      )\n",
       "      (2): ResnetBlock(\n",
       "        (block1): Sequential(\n",
       "          (0): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
       "          (1): SiLU()\n",
       "          (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (block2): Sequential(\n",
       "          (0): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
       "          (1): SiLU()\n",
       "          (2): Dropout2d(p=0.0, inplace=False)\n",
       "          (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (up): Sequential(\n",
       "      (0): Sequential(\n",
       "        (0): ResnetBlock(\n",
       "          (block1): Sequential(\n",
       "            (0): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
       "            (1): SiLU()\n",
       "            (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          )\n",
       "          (block2): Sequential(\n",
       "            (0): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
       "            (1): SiLU()\n",
       "            (2): Dropout2d(p=0.0, inplace=False)\n",
       "            (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          )\n",
       "        )\n",
       "        (1): AttnBlock(\n",
       "          (norm): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
       "          (q): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (k): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (v): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (proj_out): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "        (2): ResnetBlock(\n",
       "          (block1): Sequential(\n",
       "            (0): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
       "            (1): SiLU()\n",
       "            (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          )\n",
       "          (block2): Sequential(\n",
       "            (0): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
       "            (1): SiLU()\n",
       "            (2): Dropout2d(p=0.0, inplace=False)\n",
       "            (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          )\n",
       "        )\n",
       "        (3): AttnBlock(\n",
       "          (norm): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
       "          (q): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (k): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (v): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (proj_out): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "        (4): ResnetBlock(\n",
       "          (block1): Sequential(\n",
       "            (0): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
       "            (1): SiLU()\n",
       "            (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          )\n",
       "          (block2): Sequential(\n",
       "            (0): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
       "            (1): SiLU()\n",
       "            (2): Dropout2d(p=0.0, inplace=False)\n",
       "            (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          )\n",
       "        )\n",
       "        (5): AttnBlock(\n",
       "          (norm): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
       "          (q): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (k): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (v): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (proj_out): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "        (6): Upsample(\n",
       "          (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "      )\n",
       "      (1): Sequential(\n",
       "        (0): ResnetBlock(\n",
       "          (block1): Sequential(\n",
       "            (0): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
       "            (1): SiLU()\n",
       "            (2): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          )\n",
       "          (block2): Sequential(\n",
       "            (0): GroupNorm(32, 128, eps=1e-05, affine=True)\n",
       "            (1): SiLU()\n",
       "            (2): Dropout2d(p=0.0, inplace=False)\n",
       "            (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          )\n",
       "          (shortcut): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "        (1): ResnetBlock(\n",
       "          (block1): Sequential(\n",
       "            (0): GroupNorm(32, 128, eps=1e-05, affine=True)\n",
       "            (1): SiLU()\n",
       "            (2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          )\n",
       "          (block2): Sequential(\n",
       "            (0): GroupNorm(32, 128, eps=1e-05, affine=True)\n",
       "            (1): SiLU()\n",
       "            (2): Dropout2d(p=0.0, inplace=False)\n",
       "            (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          )\n",
       "        )\n",
       "        (2): ResnetBlock(\n",
       "          (block1): Sequential(\n",
       "            (0): GroupNorm(32, 128, eps=1e-05, affine=True)\n",
       "            (1): SiLU()\n",
       "            (2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          )\n",
       "          (block2): Sequential(\n",
       "            (0): GroupNorm(32, 128, eps=1e-05, affine=True)\n",
       "            (1): SiLU()\n",
       "            (2): Dropout2d(p=0.0, inplace=False)\n",
       "            (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          )\n",
       "        )\n",
       "        (3): Upsample(\n",
       "          (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "      )\n",
       "      (2): Sequential(\n",
       "        (0): ResnetBlock(\n",
       "          (block1): Sequential(\n",
       "            (0): GroupNorm(32, 128, eps=1e-05, affine=True)\n",
       "            (1): SiLU()\n",
       "            (2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          )\n",
       "          (block2): Sequential(\n",
       "            (0): GroupNorm(32, 128, eps=1e-05, affine=True)\n",
       "            (1): SiLU()\n",
       "            (2): Dropout2d(p=0.0, inplace=False)\n",
       "            (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          )\n",
       "        )\n",
       "        (1): ResnetBlock(\n",
       "          (block1): Sequential(\n",
       "            (0): GroupNorm(32, 128, eps=1e-05, affine=True)\n",
       "            (1): SiLU()\n",
       "            (2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          )\n",
       "          (block2): Sequential(\n",
       "            (0): GroupNorm(32, 128, eps=1e-05, affine=True)\n",
       "            (1): SiLU()\n",
       "            (2): Dropout2d(p=0.0, inplace=False)\n",
       "            (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          )\n",
       "        )\n",
       "        (2): ResnetBlock(\n",
       "          (block1): Sequential(\n",
       "            (0): GroupNorm(32, 128, eps=1e-05, affine=True)\n",
       "            (1): SiLU()\n",
       "            (2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          )\n",
       "          (block2): Sequential(\n",
       "            (0): GroupNorm(32, 128, eps=1e-05, affine=True)\n",
       "            (1): SiLU()\n",
       "            (2): Dropout2d(p=0.0, inplace=False)\n",
       "            (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          )\n",
       "        )\n",
       "        (3): Upsample(\n",
       "          (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "      )\n",
       "      (3): Sequential(\n",
       "        (0): ResnetBlock(\n",
       "          (block1): Sequential(\n",
       "            (0): GroupNorm(32, 128, eps=1e-05, affine=True)\n",
       "            (1): SiLU()\n",
       "            (2): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          )\n",
       "          (block2): Sequential(\n",
       "            (0): GroupNorm(32, 64, eps=1e-05, affine=True)\n",
       "            (1): SiLU()\n",
       "            (2): Dropout2d(p=0.0, inplace=False)\n",
       "            (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          )\n",
       "          (shortcut): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "        (1): ResnetBlock(\n",
       "          (block1): Sequential(\n",
       "            (0): GroupNorm(32, 64, eps=1e-05, affine=True)\n",
       "            (1): SiLU()\n",
       "            (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          )\n",
       "          (block2): Sequential(\n",
       "            (0): GroupNorm(32, 64, eps=1e-05, affine=True)\n",
       "            (1): SiLU()\n",
       "            (2): Dropout2d(p=0.0, inplace=False)\n",
       "            (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          )\n",
       "        )\n",
       "        (2): ResnetBlock(\n",
       "          (block1): Sequential(\n",
       "            (0): GroupNorm(32, 64, eps=1e-05, affine=True)\n",
       "            (1): SiLU()\n",
       "            (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          )\n",
       "          (block2): Sequential(\n",
       "            (0): GroupNorm(32, 64, eps=1e-05, affine=True)\n",
       "            (1): SiLU()\n",
       "            (2): Dropout2d(p=0.0, inplace=False)\n",
       "            (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          )\n",
       "        )\n",
       "        (3): Upsample(\n",
       "          (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "      )\n",
       "      (4): Sequential(\n",
       "        (0): ResnetBlock(\n",
       "          (block1): Sequential(\n",
       "            (0): GroupNorm(32, 64, eps=1e-05, affine=True)\n",
       "            (1): SiLU()\n",
       "            (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          )\n",
       "          (block2): Sequential(\n",
       "            (0): GroupNorm(32, 64, eps=1e-05, affine=True)\n",
       "            (1): SiLU()\n",
       "            (2): Dropout2d(p=0.0, inplace=False)\n",
       "            (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          )\n",
       "        )\n",
       "        (1): ResnetBlock(\n",
       "          (block1): Sequential(\n",
       "            (0): GroupNorm(32, 64, eps=1e-05, affine=True)\n",
       "            (1): SiLU()\n",
       "            (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          )\n",
       "          (block2): Sequential(\n",
       "            (0): GroupNorm(32, 64, eps=1e-05, affine=True)\n",
       "            (1): SiLU()\n",
       "            (2): Dropout2d(p=0.0, inplace=False)\n",
       "            (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          )\n",
       "        )\n",
       "        (2): ResnetBlock(\n",
       "          (block1): Sequential(\n",
       "            (0): GroupNorm(32, 64, eps=1e-05, affine=True)\n",
       "            (1): SiLU()\n",
       "            (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          )\n",
       "          (block2): Sequential(\n",
       "            (0): GroupNorm(32, 64, eps=1e-05, affine=True)\n",
       "            (1): SiLU()\n",
       "            (2): Dropout2d(p=0.0, inplace=False)\n",
       "            (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (end): Sequential(\n",
       "      (0): GroupNorm(32, 64, eps=1e-05, affine=True)\n",
       "      (1): SiLU()\n",
       "      (2): Conv2d(64, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    )\n",
       "  )\n",
       "  (quantize): VectorQuantizer2(\n",
       "    (embedding): Embedding(256, 8)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = VQVAE().to(DEVICE)\n",
    "net = torch.load(f'model_ckpt/VQVAE/epoch_AE_81.pt')\n",
    "net.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ind_data(net, dataloader):\n",
    "    indices = []\n",
    "    for now_step, batch_data in tqdm(enumerate(dataloader), total=len(dataloader)):\n",
    "        raw_img, seg_img, brain_idx, z_idx = [\n",
    "            data.to(DEVICE) for data in batch_data]\n",
    "        with torch.no_grad():\n",
    "            batch_size = raw_img.shape[0]\n",
    "            latent = net.encode(raw_img)\n",
    "            quant, diff_loss, (_, _, ind) = net.quantize(latent)\n",
    "            ind = rearrange(ind, '(b c h w) -> b c h w', b=batch_size,\n",
    "                            h=net.z_shape[0], w=net.z_shape[1])\n",
    "            indices.append(ind.detach())\n",
    "    indices = torch.cat(indices)\n",
    "    indices = reduce(indices, 'b c h w -> b h w', 'min')\n",
    "    return indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 337/337 [01:17<00:00,  4.37it/s]\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    ind = get_ind_data(net, dataloader).detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10784, 16, 16])\n",
      "tensor(203, device='cuda:0') tensor(14, device='cuda:0')\n",
      "   9   7   9   8   9  11   9  11   9   8  11   8   7  10  10   9\n",
      "   5   4  10  10  11   7   8   9  10  10   8   6   6  10   9   9\n",
      "   9   6  10  11   9  11  10  10   9   9   7  10  11  10   9   6\n",
      "  10   7  10  11  11  10  11   9  10   8   8  10   7  11  11  11\n",
      "  11   9   9   9   7   8   9   9   9   8   9   8  10   9   8   9\n",
      "  10   9   9   8   9   9   8   6   8  10   6   8   8   9  11  10\n",
      "  10   8  10  10   9   7   7   8   8   9   7   9   8   8   9   7\n",
      "  10   9   9   9  11   9   9   8   8   7   9  10   8   9   9   8\n",
      "   8   8   7   8  10   8   8   8   7   6   7   7   8   8  10   7\n",
      "  11   8   7   9   7   7   8   5   6   7   7   9   8   7   7   7\n",
      "   7   8   6  11   6   8   9   8   8   7   8  10   9   8   8  10\n",
      "   7   8   9   7  10   8   9   7  10  11   9   8   7  10  10  11\n",
      "  10   8  10   9   7   9   8   8   7  10   8   9   8  10  10   9\n",
      "  11  10   7  11   8   7   9   9   8   9  10   9  10   9  10  10\n",
      "  11   8  11   9   7  10  11  10  10   8   9   9  12  10   9  11\n",
      "  11  10  11  11  11  11  11  11  12  11  11  11   9  10  10  10\n"
     ]
    }
   ],
   "source": [
    "print(ind.shape)\n",
    "print(torch.max(ind), torch.min(ind))\n",
    "for i in range(16):\n",
    "    for j in range(16):\n",
    "        x = ind[:, i, j]\n",
    "        print('%4d' % len(torch.unique(x)), end='')\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10784, 16, 16])\n",
      "torch.Size([10784, 16, 16, 8])\n"
     ]
    }
   ],
   "source": [
    "# ind = rearrange(ind[:27], 'b w h -> b w h ')\n",
    "print(ind.shape)\n",
    "# ind shape: [batch_size, w*h]\n",
    "emb = net.quantize.embedding(ind)\n",
    "# emb shape: [batch_size, w*h, emb_len]\n",
    "print(emb.shape)\n",
    "\n",
    "ind = rearrange(ind, 'N W H -> N (W H)')\n",
    "emb = rearrange(emb, 'N W H E -> N ( W H ) E')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "class PositionalEncoding(nn.Module):\n",
    "    \"Implement the PE function.\"\n",
    "\n",
    "    def __init__(self, d_model, dropout, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        # 初始化Shape为(max_len, d_model)的PE (positional encoding)\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        # 初始化一个tensor [[0, 1, 2, 3, ...]]\n",
    "        position = torch.arange(0, max_len).unsqueeze(1)\n",
    "        # 这里就是sin和cos括号中的内容，通过e和ln进行了变换\n",
    "        div_term = torch.exp(\n",
    "            torch.arange(0, d_model, 2) * -(math.log(10000.0) / d_model)\n",
    "        )\n",
    "        # 计算PE(pos, 2i)\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        # 计算PE(pos, 2i+1)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        # 为了方便计算，在最外面在unsqueeze出一个batch\n",
    "        pe = pe.unsqueeze(0)\n",
    "        # 如果一个参数不参与梯度下降，但又希望保存model的时候将其保存下来\n",
    "        # 这个时候就可以用register_buffer\n",
    "        self.register_buffer(\"pe\", pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x 为embedding后的inputs，例如(1,7, 128)，batch size为1,7个单词，单词维度为128\n",
    "        \"\"\"\n",
    "        # 将x和positional encoding相加。\n",
    "        x = x + self.pe[:, : x.size(1)].requires_grad_(False)\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PixelCNN(nn.Module):\n",
    "\n",
    "    def __init__(self, vae):\n",
    "        super(PixelCNN, self).__init__()\n",
    "\n",
    "        self.embed_dim = vae.quantize.e_dim\n",
    "        self.n_embed = vae.quantize.n_e\n",
    "        self.z_shape = vae.z_shape\n",
    "        self.transformer = nn.Transformer(d_model=self.embed_dim,\n",
    "                                     nhead=8,\n",
    "                                     num_encoder_layers=6,\n",
    "                                     num_decoder_layers=6,\n",
    "                                     dim_feedforward=256,\n",
    "                                     dropout=0.1,\n",
    "                                     batch_first=True).cuda()\n",
    "        \n",
    "        self.positional_encoding = PositionalEncoding(d_model=self.embed_dim, max_len=self.z_shape[0] * self.z_shape[1], dropout=0)\n",
    "\n",
    "        self.predictor = nn.Linear(self.embed_dim, self.n_embed)\n",
    "\n",
    "    def generate_random_mask(self, batch_size, S, low=None, high=None):\n",
    "        if low is None:\n",
    "            low = 0\n",
    "        if high is None:\n",
    "            high = S+1\n",
    "        mask = torch.zeros([batch_size, S], device=DEVICE, dtype=torch.long)\n",
    "        for i in range(batch_size):\n",
    "            mask_num = torch.randint(low=low, high=high, size=(1,),  device=DEVICE)\n",
    "            perm = torch.randperm(S, device=DEVICE)[None, :]\n",
    "            mask[i, :] = (perm < mask_num).long()\n",
    "        return mask\n",
    "\n",
    "    def forward(self, src, low=None, high=None):\n",
    "        N, S, E = src.shape\n",
    "        tgt_mask = nn.Transformer.generate_square_subsequent_mask(S).cuda()\n",
    "        from einops import repeat, rearrange\n",
    "\n",
    "        # tgt = src\n",
    "        src_mask = self.generate_random_mask(batch_size=N, S=S, low=low, high=high)\n",
    "        src = src * (1 - src_mask[:, :, None].long())\n",
    "        src = self.positional_encoding(src)\n",
    "        tgt = src\n",
    "        # tgt = self.positional_encoding(tgt)\n",
    "        # print(src_mask, tgt_mask)\n",
    "        # src = rearrange(src, 'N S E -> ( N S ) E')\n",
    "        # print(src_mask.shape, tgt_mask.shape)\n",
    "        out = self.transformer(src, tgt, tgt_mask=tgt_mask)\n",
    "        out = self.predictor(out)\n",
    "        # out = rearrange(out, '(N S) NE -> N S NE', N=N, S=S)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "pixelCNN = PixelCNN(net).to(DEVICE)\n",
    "from torchinfo import summary\n",
    "criteria = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(pixelCNN.parameters(), lr=2e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10784, 256]) torch.Size([10784, 256, 8])\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "print(ind.shape, emb.shape)\n",
    "dataset = TensorDataset(emb, ind)\n",
    "dataloader = DataLoader(dataset=dataset, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(i):\n",
    "\n",
    "    cut = min(i // 2, 15)\n",
    "    low, high = cut*16, (cut+1) * 16\n",
    "    print(low, high)\n",
    "    total_loss = 0\n",
    "    for now_step, batch_data in tqdm(enumerate(dataloader), total=len(dataloader)):\n",
    "        emb, ind = [data.to(DEVICE) for data in batch_data]\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        out = pixelCNN(emb.detach(), low=low, high=high)\n",
    "        loss = criteria(out.contiguous().view(-1, out.size(-1)), ind.contiguous().view(-1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "    return total_loss / len(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 337/337 [00:44<00:00,  7.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0: loss: 4.062723052961537\n",
      "0 16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 337/337 [00:45<00:00,  7.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1: loss: 2.7014058704546016\n",
      "16 32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 337/337 [00:45<00:00,  7.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 2: loss: 1.5812045509100667\n",
      "16 32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 337/337 [00:45<00:00,  7.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 3: loss: 0.8754756072154738\n",
      "32 48\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 337/337 [00:45<00:00,  7.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 4: loss: 0.6459840102790018\n",
      "32 48\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 337/337 [00:45<00:00,  7.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 5: loss: 0.48777048223450203\n",
      "48 64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 337/337 [00:45<00:00,  7.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 6: loss: 0.4987748389958628\n",
      "48 64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 337/337 [00:45<00:00,  7.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 7: loss: 0.45034356888394683\n",
      "64 80\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 337/337 [00:45<00:00,  7.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 8: loss: 0.515927288493346\n",
      "64 80\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 337/337 [00:45<00:00,  7.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 9: loss: 0.49768638504718815\n",
      "80 96\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 337/337 [00:45<00:00,  7.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 10: loss: 0.5790891076053993\n",
      "80 96\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 337/337 [00:45<00:00,  7.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 11: loss: 0.5699333394670345\n",
      "96 112\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 337/337 [00:45<00:00,  7.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 12: loss: 0.6579870445438238\n",
      "96 112\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 337/337 [00:45<00:00,  7.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 13: loss: 0.6520011608961428\n",
      "112 128\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 337/337 [00:45<00:00,  7.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 14: loss: 0.7394130466953583\n",
      "112 128\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 337/337 [00:46<00:00,  7.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 15: loss: 0.7344899487070938\n",
      "128 144\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 337/337 [00:48<00:00,  7.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 16: loss: 0.8236545984044626\n",
      "128 144\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 337/337 [00:46<00:00,  7.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 17: loss: 0.8193950485051209\n",
      "144 160\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 337/337 [00:45<00:00,  7.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 18: loss: 0.9084536404920969\n",
      "144 160\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 337/337 [00:45<00:00,  7.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 19: loss: 0.9042048832783006\n",
      "160 176\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 337/337 [00:45<00:00,  7.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 20: loss: 0.9921920198715052\n",
      "160 176\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 337/337 [00:45<00:00,  7.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 21: loss: 0.9894592620498702\n",
      "176 192\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 337/337 [00:45<00:00,  7.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 22: loss: 1.0781636524624925\n",
      "176 192\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 337/337 [00:45<00:00,  7.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 23: loss: 1.074659144489277\n",
      "192 208\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 337/337 [00:45<00:00,  7.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 24: loss: 1.164035652440804\n",
      "192 208\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 337/337 [00:45<00:00,  7.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 25: loss: 1.1614157315177804\n",
      "208 224\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 337/337 [00:45<00:00,  7.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 26: loss: 1.2499922407840762\n",
      "208 224\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 337/337 [00:45<00:00,  7.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 27: loss: 1.2484175936050868\n",
      "224 240\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 337/337 [00:45<00:00,  7.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 28: loss: 1.3376270472472782\n",
      "224 240\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 337/337 [00:45<00:00,  7.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 29: loss: 1.3353553684953412\n",
      "240 256\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 337/337 [00:45<00:00,  7.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 30: loss: 1.4239959939651037\n",
      "240 256\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 337/337 [00:45<00:00,  7.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 31: loss: 1.4215403798779678\n",
      "240 256\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 337/337 [00:45<00:00,  7.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 32: loss: 1.419804951203683\n",
      "240 256\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 337/337 [00:45<00:00,  7.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 33: loss: 1.41830621381188\n",
      "240 256\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 337/337 [00:45<00:00,  7.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 34: loss: 1.4156483280906338\n",
      "240 256\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 337/337 [00:45<00:00,  7.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 35: loss: 1.4141214923264365\n",
      "240 256\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 337/337 [00:45<00:00,  7.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 36: loss: 1.41331833264948\n",
      "240 256\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 337/337 [00:45<00:00,  7.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 37: loss: 1.4110171844414505\n",
      "240 256\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 337/337 [00:45<00:00,  7.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 38: loss: 1.4107090181695956\n",
      "240 256\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 337/337 [00:45<00:00,  7.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 39: loss: 1.4095486611216288\n",
      "240 256\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 337/337 [00:45<00:00,  7.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 40: loss: 1.4084605930113296\n",
      "240 256\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 337/337 [00:45<00:00,  7.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 41: loss: 1.4071342329596908\n",
      "240 256\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 337/337 [00:45<00:00,  7.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 42: loss: 1.4062610949536105\n",
      "240 256\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|██▉       | 100/337 [00:14<00:34,  6.94it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32md:\\class\\tutor\\kasper_ML\\hw3\\check_ind.ipynb Cell 13\u001b[0m line \u001b[0;36m2\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/class/tutor/kasper_ML/hw3/check_ind.ipynb#X15sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m50\u001b[39m):\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/class/tutor/kasper_ML/hw3/check_ind.ipynb#X15sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mepoch \u001b[39m\u001b[39m{\u001b[39;00mi\u001b[39m}\u001b[39;00m\u001b[39m: loss: \u001b[39m\u001b[39m{\u001b[39;00mtrain(i)\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "\u001b[1;32md:\\class\\tutor\\kasper_ML\\hw3\\check_ind.ipynb Cell 13\u001b[0m line \u001b[0;36m1\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/class/tutor/kasper_ML/hw3/check_ind.ipynb#X15sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m     loss \u001b[39m=\u001b[39m criteria(out\u001b[39m.\u001b[39mcontiguous()\u001b[39m.\u001b[39mview(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, out\u001b[39m.\u001b[39msize(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)), ind\u001b[39m.\u001b[39mcontiguous()\u001b[39m.\u001b[39mview(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m))\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/class/tutor/kasper_ML/hw3/check_ind.ipynb#X15sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m     loss\u001b[39m.\u001b[39mbackward()\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/class/tutor/kasper_ML/hw3/check_ind.ipynb#X15sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m     optimizer\u001b[39m.\u001b[39;49mstep()\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/class/tutor/kasper_ML/hw3/check_ind.ipynb#X15sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m     total_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m loss\u001b[39m.\u001b[39mitem()\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/class/tutor/kasper_ML/hw3/check_ind.ipynb#X15sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m \u001b[39mreturn\u001b[39;00m total_loss \u001b[39m/\u001b[39m \u001b[39mlen\u001b[39m(dataloader)\n",
      "File \u001b[1;32mc:\\Python39\\lib\\site-packages\\torch\\optim\\optimizer.py:88\u001b[0m, in \u001b[0;36mOptimizer._hook_for_profile.<locals>.profile_hook_step.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     86\u001b[0m profile_name \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mOptimizer.step#\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m.step\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(obj\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m)\n\u001b[0;32m     87\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mautograd\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mrecord_function(profile_name):\n\u001b[1;32m---> 88\u001b[0m     \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Python39\\lib\\site-packages\\torch\\autograd\\grad_mode.py:27\u001b[0m, in \u001b[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[0;32m     25\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m     26\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclone():\n\u001b[1;32m---> 27\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Python39\\lib\\site-packages\\torch\\optim\\adam.py:141\u001b[0m, in \u001b[0;36mAdam.step\u001b[1;34m(self, closure)\u001b[0m\n\u001b[0;32m    138\u001b[0m             \u001b[39m# record the step after step update\u001b[39;00m\n\u001b[0;32m    139\u001b[0m             state_steps\u001b[39m.\u001b[39mappend(state[\u001b[39m'\u001b[39m\u001b[39mstep\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[1;32m--> 141\u001b[0m     F\u001b[39m.\u001b[39;49madam(params_with_grad,\n\u001b[0;32m    142\u001b[0m            grads,\n\u001b[0;32m    143\u001b[0m            exp_avgs,\n\u001b[0;32m    144\u001b[0m            exp_avg_sqs,\n\u001b[0;32m    145\u001b[0m            max_exp_avg_sqs,\n\u001b[0;32m    146\u001b[0m            state_steps,\n\u001b[0;32m    147\u001b[0m            amsgrad\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mamsgrad\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    148\u001b[0m            beta1\u001b[39m=\u001b[39;49mbeta1,\n\u001b[0;32m    149\u001b[0m            beta2\u001b[39m=\u001b[39;49mbeta2,\n\u001b[0;32m    150\u001b[0m            lr\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mlr\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    151\u001b[0m            weight_decay\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mweight_decay\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    152\u001b[0m            eps\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39meps\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    153\u001b[0m            maximize\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mmaximize\u001b[39;49m\u001b[39m'\u001b[39;49m])\n\u001b[0;32m    154\u001b[0m \u001b[39mreturn\u001b[39;00m loss\n",
      "File \u001b[1;32mc:\\Python39\\lib\\site-packages\\torch\\optim\\_functional.py:110\u001b[0m, in \u001b[0;36madam\u001b[1;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[0;32m    105\u001b[0m     denom \u001b[39m=\u001b[39m (exp_avg_sq\u001b[39m.\u001b[39msqrt() \u001b[39m/\u001b[39m math\u001b[39m.\u001b[39msqrt(bias_correction2))\u001b[39m.\u001b[39madd_(eps)\n\u001b[0;32m    109\u001b[0m step_size \u001b[39m=\u001b[39m lr \u001b[39m/\u001b[39m bias_correction1\n\u001b[1;32m--> 110\u001b[0m param\u001b[39m.\u001b[39;49maddcdiv_(exp_avg, denom, value\u001b[39m=\u001b[39;49m\u001b[39m-\u001b[39;49mstep_size)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for i in range(50):\n",
    "    print(f\"epoch {i}: loss: {train(i)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0\t1 0\t2 0\t3 0\t4 0\t5 0\t6 0\t7 0\t8 0\t9 0\t10 0\t11 0\t12 0\t13 0\t14 557\t15 0\t\n",
      "16 0\t17 0\t18 0\t19 0\t20 0\t21 0\t22 0\t23 0\t24 0\t25 0\t26 0\t27 0\t28 0\t29 0\t30 0\t31 0\t\n",
      "32 0\t33 0\t34 0\t35 0\t36 0\t37 0\t38 0\t39 0\t40 0\t41 0\t42 0\t43 0\t44 0\t45 0\t46 0\t47 641\t\n",
      "48 0\t49 0\t50 0\t51 0\t52 0\t53 0\t54 0\t55 1552\t56 0\t57 0\t58 0\t59 0\t60 0\t61 0\t62 0\t63 0\t\n",
      "64 0\t65 0\t66 0\t67 0\t68 0\t69 3\t70 0\t71 0\t72 0\t73 0\t74 0\t75 0\t76 0\t77 0\t78 0\t79 0\t\n",
      "80 622\t81 0\t82 0\t83 856126\t84 0\t85 0\t86 7182\t87 0\t88 0\t89 0\t90 0\t91 0\t92 0\t93 471353\t94 0\t95 0\t\n",
      "96 0\t97 0\t98 0\t99 0\t100 0\t101 0\t102 0\t103 0\t104 0\t105 0\t106 0\t107 0\t108 0\t109 0\t110 0\t111 0\t\n",
      "112 0\t113 0\t114 0\t115 0\t116 0\t117 0\t118 0\t119 0\t120 0\t121 0\t122 0\t123 0\t124 0\t125 0\t126 0\t127 0\t\n",
      "128 0\t129 0\t130 0\t131 0\t132 439468\t133 0\t134 0\t135 0\t136 0\t137 0\t138 0\t139 0\t140 0\t141 0\t142 0\t143 0\t\n",
      "144 0\t145 0\t146 577\t147 0\t148 0\t149 0\t150 651409\t151 0\t152 0\t153 0\t154 0\t155 0\t156 0\t157 0\t158 0\t159 0\t\n",
      "160 0\t161 0\t162 0\t163 0\t164 0\t165 0\t166 0\t167 0\t168 0\t169 0\t170 0\t171 0\t172 0\t173 0\t174 0\t175 0\t\n",
      "176 0\t177 0\t178 0\t179 0\t180 0\t181 0\t182 0\t183 0\t184 0\t185 0\t186 0\t187 0\t188 0\t189 0\t190 0\t191 0\t\n",
      "192 0\t193 0\t194 0\t195 0\t196 0\t197 0\t198 0\t199 0\t200 0\t201 0\t202 0\t203 331214\t"
     ]
    }
   ],
   "source": [
    "\n",
    "binc = torch.bincount(ind.view([-1]))\n",
    "for i, c in enumerate(binc):\n",
    "    print(i, c.item(), end='\\t')\n",
    "    if i % 16 == 15:\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 93, 132, 150,  93,  83,  93, 132, 150, 203,  83,  93,  93, 132, 150,\n",
      "        150,  83, 132,  93, 132, 132, 150,  83,  93, 150,  83,  93,  83,  83,\n",
      "        132, 132, 132, 132, 150,  93,  93,  83, 203,  83, 132, 150, 150, 132,\n",
      "         83,  83,  83,  83, 132, 132, 132,  83, 150, 150,  83,  83,  83, 203,\n",
      "        150,  83,  83, 132,  83,  83,  83, 150,  93,  83, 132, 132, 150, 150,\n",
      "         83, 132, 150,  83,  93, 132, 150, 150,  83,  83, 150,  83,  83, 150,\n",
      "        150, 132,  83, 132, 132,  83,  83, 203,  83, 203, 132,  83, 132, 150,\n",
      "         83, 150, 132,  93,  83,  83, 150, 150, 150, 150, 150,  83, 150, 132,\n",
      "         83, 150, 150, 132,  83,  83, 203, 203, 203,  83,  93,  83, 203,  93,\n",
      "        203, 150,  83, 150, 150, 132, 203, 203, 203,  83, 203,  83,  83, 203,\n",
      "        203, 203, 132, 203, 203,  93,  83, 150, 150, 150, 150, 150, 203, 150,\n",
      "        132, 203, 132,  83, 132, 203, 150,  83,  83,  93,  83,  83, 132,  83,\n",
      "        203,  83,  83,  93, 132, 132,  93,  93,  93,  83, 150, 150, 203,  83,\n",
      "        150, 150, 132, 132,  83, 203, 150,  93,  93, 132,  83,  83,  83,  83,\n",
      "         83, 203,  93,  83,  93, 203, 150, 150,  93,  83, 132, 150, 150, 150,\n",
      "        150, 203,  83, 203,  83,  83,  83,  83,  83,  83,  83, 203, 150, 203,\n",
      "         93, 150,  93,  83, 150, 150, 150, 150, 150, 132,  83, 150, 150, 150,\n",
      "        132,  83,  93,  93, 150,  93,  83,  83,  83,  83,  83, 132,  93,  83,\n",
      "         83,  83,  83,  86], device='cuda:0')\n",
      "tensor([ 93, 132, 150,  93,  83,  93, 132, 150, 203,  83,  93,  93, 132, 150,\n",
      "        150,  83, 132,  93, 132, 132, 150,  83,  93, 150,  83,  93,  83,  83,\n",
      "        132, 132, 132, 132, 203, 132,  86,  86,  86,  86,  86,  86,  86, 132,\n",
      "         86, 203, 150,  86,  86,  86, 150, 150, 150, 150, 150,  86, 150,  14,\n",
      "         80,  80, 150, 203, 150,  80,  80,  55, 132, 150,  80,  80,  14,  55,\n",
      "         55,  55, 150,  80,  14,  55,  55,  55,  55,  55,  55,  55,  55,  55,\n",
      "        150,  55,  55,  55, 150, 150, 132, 150, 150,  55, 150, 132, 150, 150,\n",
      "        150,  80,  80, 150, 150, 150, 150, 150, 150,  86, 150, 132, 150, 150,\n",
      "        150,  80, 150, 150, 150,  86,  14,  14, 150, 132, 150,  14,  80,  14,\n",
      "        150, 150, 150,  14,  55,  14,  55,  55,  55, 150,  55,  55,  55,  55,\n",
      "         55, 132,  55,  55,  55,  55,  55,  55,  55,  55,  14,  55,  55,  55,\n",
      "        150,  14,  14, 150,  14, 132, 150,  14,  14,  80,  80, 150, 132, 150,\n",
      "         80,  14,  14,  14, 150, 150, 150,  14,  80, 150, 150, 150,  14,  80,\n",
      "         14,  80, 150,  55,  80,  80,  55,  47,  55,  55,  55,  80,  55,  55,\n",
      "         55, 132,  55,  55,  55,  55,  55,  55,  55,  55,  55,  55,  55,  55,\n",
      "         55,  14,  55,  55,  55,  55,  55,  55,  80,  55,  55, 150,  14,  55,\n",
      "         80,  14,  14,  14, 132, 132,  80,  14,  14,  14, 132, 132,  14,  14,\n",
      "         80,  14,  55,  14,  14,  14,  80,  55,  55,  55,  14,  80,  14,  14,\n",
      "         14,  47,  55,  14], device='cuda:0')\n",
      "torch.Size([32, 1, 256, 256])\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    \n",
    "    # emb = rearrange(emb, 'N W H E -> N ( W H ) E')\n",
    "    # cur = torch.zeros(32, 16 * 16, 8).to(DEVICE)\n",
    "    cur = net.quantize.embedding(ind[:32])\n",
    "    cur[:, 32:, :] = 0\n",
    "    # ans = net.quantize.embedding(ind[:64:2])\n",
    "    print(ind[0])\n",
    "    N, S, E = 32, 16*16, 8\n",
    "    tgt_mask = nn.Transformer.generate_square_subsequent_mask(S).cuda()\n",
    "    from einops import repeat, rearrange\n",
    "    \n",
    "    # cur = torch.zeros([N, S, E], device=DEVICE)\n",
    "\n",
    "    for i in range(256+1):\n",
    "        # print(cur[0, :,0])\n",
    "        pos_cur = pixelCNN.positional_encoding(cur)\n",
    "        out = pixelCNN.transformer(pos_cur, pos_cur, tgt_mask=tgt_mask)\n",
    "        out_ind = torch.argmax(pixelCNN.predictor(out[:, :i, :]), dim=2)\n",
    "        out_emb = net.quantize.embedding(out_ind)\n",
    "        cur[:, :i, :] = out_emb\n",
    "        # print(out_emb[:, None, :].shape)\n",
    "        # cur [:, i, :] = \n",
    "    # nxt_ind = torch.argmax(pixelCNN(net.quantize.embedding(ind[:32])), dim=2)\n",
    "    print(out_ind[0])\n",
    "    z_q = net.quantize.embedding(out_ind)\n",
    "    z_q = rearrange(z_q, 'b (w h) c -> b c w h', w = 16)\n",
    "    sample = net.decode(z_q).cpu()\n",
    "\n",
    "    print(sample.shape)\n",
    "    from util import compact_large_image\n",
    "    imgs = compact_large_image(sample, HZ=4, WZ=8)\n",
    "    for idx in range(imgs.shape[0]):\n",
    "        plt.imsave('test.png', imgs[0] * 0.5 + 0.5, cmap='gray')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
